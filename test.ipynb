{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/rfuk0503/myPractice/blob/master/test.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "Hx-kkReYJ4IK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7abdd682-af37-41e3-9312-4ae9dfa7b358"
      },
      "cell_type": "code",
      "source": [
        "print(\"Hello World!\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello World!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qRcj76weKK2g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8357cc22-33d4-46c8-d5a7-0b8b3e64f1f7"
      },
      "cell_type": "code",
      "source": [
        "import keras"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "DnkrvjUCKQdz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4jvMpjZfKyUB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1053e6c4-07bf-47f4-cad5-69200740235a"
      },
      "cell_type": "code",
      "source": [
        "print(tensorflow.test.is_built_with_cuda())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mv_9eGkfK2fD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4258389b-745d-45fa-a7ba-1df1f25101a4"
      },
      "cell_type": "code",
      "source": [
        "print(tensorflow.test.is_built_with_cuda())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8_srsBdZK5TZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "d5e39587-f8a3-446d-8f24-fb3ec33d2ffd"
      },
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-eb42ca6e4af3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo install torch, click the button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "2QsVYPvqLKox",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a8bbf6ac-65ec-4d2e-c445-853d4d8e0fcf"
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oxYfTV0zLOsh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e2f706e9-ddcc-4ad5-a6e5-f841d1166a5f"
      },
      "cell_type": "code",
      "source": [
        "!ls -l"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 4\n",
            "drwxr-xr-x 2 root root 4096 Sep 28 23:32 sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p7Pp8KOJLQuq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "88ade25a-ea86-498b-89a0-eb837fa90586"
      },
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_lgEmI4iLUVj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c6c0d12a-73f7-4a2c-efa4-020869c55de2"
      },
      "cell_type": "code",
      "source": [
        "!hostname"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "051dce5eca9c\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rvVv5vrGLWdF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e997456b-b2de-4d97-9d4c-2768e9151068"
      },
      "cell_type": "code",
      "source": [
        "!uname -a"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linux 051dce5eca9c 4.14.33+ #1 SMP Sat Aug 11 08:05:16 PDT 2018 x86_64 x86_64 x86_64 GNU/Linux\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2T6jI5u9Le9a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "879f2653-05cb-41fa-c76c-4b532de927e0"
      },
      "cell_type": "code",
      "source": [
        "!ps aux"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\n",
            "root         1  0.0  0.0  39008  6352 ?        Ss   06:54   0:00 /bin/bash -e /d\n",
            "root         9  0.1  0.3 677324 50500 ?        Sl   06:54   0:00 node /tools/nod\n",
            "root        31  0.1  0.3 678132 44364 ?        Sl   06:54   0:00 /tools/node/bin\n",
            "root        55  0.3  0.4 185228 57536 ?        Sl   06:54   0:01 /usr/bin/python\n",
            "root        62  0.9  1.6 918032 225036 ?       Ssl  06:55   0:03 /usr/bin/python\n",
            "root        84  0.0  0.1  54024 14760 ?        S    06:55   0:00 /usr/bin/python\n",
            "root        98  0.0  0.0  63304  6656 ?        R    07:01   0:00 ps aux\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AH7CY9U1Liwp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e55a80ea-2252-44ac-c2ab-2747eb8bc21e"
      },
      "cell_type": "code",
      "source": [
        "!awk '{BEGIN{print \"Hello AWK\"}}'"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "awk: line 1: missing } near BEGIN\n",
            "awk: line 1: extra '}'\n",
            "awk: line 1: syntax error at or near end of line\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9ZLC6e6CLxAq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0b97a829-c138-4cd6-cfcc-c04ac41be5bd"
      },
      "cell_type": "code",
      "source": [
        "!ls -l | awk '{ print $2}'"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HXEvKNLfMIxB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "098e2964-14d4-4293-d325-058e2940e96a"
      },
      "cell_type": "code",
      "source": [
        "!ls -l"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 4\n",
            "drwxr-xr-x 2 root root 4096 Sep 28 23:32 sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "R6EFczCYMLOR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3604
        },
        "outputId": "d7a5c99b-b701-4221-8e74-ab74465b0242"
      },
      "cell_type": "code",
      "source": [
        "!top"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?1h\u001b=\u001b[H\u001b[2J\u001b[mtop - 07:05:04 up 11 min,  0 users,  load average: 0.01, 0.04, 0.04\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "Tasks:\u001b[m\u001b[m\u001b[1m   7 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m   1 \u001b[m\u001b[mrunning,\u001b[m\u001b[m\u001b[1m   6 \u001b[m\u001b[msleeping,\u001b[m\u001b[m\u001b[1m   0 \u001b[m\u001b[mstopped,\u001b[m\u001b[m\u001b[1m   0 \u001b[m\u001b[mzombie\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "%Cpu(s):\u001b[m\u001b[m\u001b[1m  1.4 \u001b[m\u001b[mus,\u001b[m\u001b[m\u001b[1m  1.2 \u001b[m\u001b[msy,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mni,\u001b[m\u001b[m\u001b[1m 94.0 \u001b[m\u001b[mid,\u001b[m\u001b[m\u001b[1m  3.4 \u001b[m\u001b[mwa,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mhi,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[msi,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mst\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "KiB Mem :\u001b[m\u001b[m\u001b[1m 13335236 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m 10328344 \u001b[m\u001b[mfree,\u001b[m\u001b[m\u001b[1m   412904 \u001b[m\u001b[mused,\u001b[m\u001b[m\u001b[1m  2593988 \u001b[m\u001b[mbuff/cache\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "KiB Swap:\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mfree,\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mused.\u001b[m\u001b[m\u001b[1m 12664108 \u001b[m\u001b[mavail Mem \u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "\u001b[K\n",
            "\u001b[7m  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND     \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m    1 root      20   0   39008   6352   5092 S   0.0  0.0   0:00.09 run.sh      \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m    9 root      20   0  677324  50504  25428 S   0.0  0.4   0:00.86 node        \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m   31 root      20   0  679668  46056  25932 S   0.0  0.3   0:00.93 node        \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m   55 root      20   0  186260  57608  12216 S   0.0  0.4   0:01.44 jupyter-no+ \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m   62 root      20   0  918032 225088  79496 S   0.0  1.7   0:03.80 python3     \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m   84 root      20   0   54024  14760   7636 S   0.0  0.1   0:00.08 python3     \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m\u001b[1m  108 root      20   0   65356   7036   5280 R   0.0  0.1   0:00.01 top         \u001b[m\u001b[m\u001b[K\n",
            "\u001b[J\u001b[H\u001b[mtop - 07:05:07 up 11 min,  0 users,  load average: 0.00, 0.04, 0.04\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "Tasks:\u001b[m\u001b[m\u001b[1m   7 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m   1 \u001b[m\u001b[mrunning,\u001b[m\u001b[m\u001b[1m   6 \u001b[m\u001b[msleeping,\u001b[m\u001b[m\u001b[1m   0 \u001b[m\u001b[mstopped,\u001b[m\u001b[m\u001b[1m   0 \u001b[m\u001b[mzombie\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "%Cpu(s):\u001b[m\u001b[m\u001b[1m  0.8 \u001b[m\u001b[mus,\u001b[m\u001b[m\u001b[1m  0.2 \u001b[m\u001b[msy,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mni,\u001b[m\u001b[m\u001b[1m 98.8 \u001b[m\u001b[mid,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mwa,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mhi,\u001b[m\u001b[m\u001b[1m  0.2 \u001b[m\u001b[msi,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mst\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "KiB Mem :\u001b[m\u001b[m\u001b[1m 13335236 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m 10328008 \u001b[m\u001b[mfree,\u001b[m\u001b[m\u001b[1m   413224 \u001b[m\u001b[mused,\u001b[m\u001b[m\u001b[1m  2594004 \u001b[m\u001b[mbuff/cache\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "KiB Swap:\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mfree,\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mused.\u001b[m\u001b[m\u001b[1m 12663788 \u001b[m\u001b[mavail Mem \u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "\u001b[K\n",
            "\n",
            "\u001b[m   62 root      20   0  918032 225088  79496 S   1.0  1.7   0:03.83 python3     \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m   31 root      20   0  679668  46320  25932 S   0.3  0.3   0:00.94 node        \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m   55 root      20   0  186260  57608  12216 S   0.3  0.4   0:01.45 jupyter-no+ \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m    1 root      20   0   39008   6352   5092 S   0.0  0.0   0:00.09 run.sh      \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m    9 root      20   0  677324  50504  25428 S   0.0  0.4   0:00.86 node        \u001b[m\u001b[m\u001b[K\n",
            "\n",
            "\n",
            "\u001b[J\u001b[H\u001b[mtop - 07:05:10 up 11 min,  0 users,  load average: 0.00, 0.04, 0.04\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "\n",
            "%Cpu(s):\u001b[m\u001b[m\u001b[1m  0.8 \u001b[m\u001b[mus,\u001b[m\u001b[m\u001b[1m  0.2 \u001b[m\u001b[msy,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mni,\u001b[m\u001b[m\u001b[1m 99.0 \u001b[m\u001b[mid,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mwa,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mhi,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[msi,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mst\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "KiB Mem :\u001b[m\u001b[m\u001b[1m 13335236 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m 10328020 \u001b[m\u001b[mfree,\u001b[m\u001b[m\u001b[1m   413152 \u001b[m\u001b[mused,\u001b[m\u001b[m\u001b[1m  2594064 \u001b[m\u001b[mbuff/cache\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "KiB Swap:\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mfree,\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mused.\u001b[m\u001b[m\u001b[1m 12663852 \u001b[m\u001b[mavail Mem \u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "\u001b[K\n",
            "\n",
            "\u001b[m   62 root      20   0  918032 225100  79496 S   0.7  1.7   0:03.85 python3     \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m   31 root      20   0  679668  46320  25932 S   0.3  0.3   0:00.95 node        \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m   55 root      20   0  186260  57608  12216 S   0.3  0.4   0:01.46 jupyter-no+ \u001b[m\u001b[m\u001b[K\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[J\u001b[H\u001b[mtop - 07:05:13 up 11 min,  0 users,  load average: 0.00, 0.04, 0.04\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "\n",
            "%Cpu(s):\u001b[m\u001b[m\u001b[1m  1.0 \u001b[m\u001b[mus,\u001b[m\u001b[m\u001b[1m  0.3 \u001b[m\u001b[msy,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mni,\u001b[m\u001b[m\u001b[1m 98.7 \u001b[m\u001b[mid,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mwa,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mhi,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[msi,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mst\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "KiB Mem :\u001b[m\u001b[m\u001b[1m 13335236 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m 10328592 \u001b[m\u001b[mfree,\u001b[m\u001b[m\u001b[1m   412572 \u001b[m\u001b[mused,\u001b[m\u001b[m\u001b[1m  2594072 \u001b[m\u001b[mbuff/cache\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "KiB Swap:\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mfree,\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mused.\u001b[m\u001b[m\u001b[1m 12664904 \u001b[m\u001b[mavail Mem \u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "\u001b[K\n",
            "\n",
            "\u001b[m   31 root      20   0  680692  46072  25932 S   0.7  0.3   0:00.97 node        \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m   55 root      20   0  186260  57612  12216 S   0.3  0.4   0:01.47 jupyter-no+ \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m   62 root      20   0  918032 225100  79496 S   0.3  1.7   0:03.86 python3     \u001b[m\u001b[m\u001b[K\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[J\u001b[H\u001b[mtop - 07:05:16 up 11 min,  0 users,  load average: 0.00, 0.04, 0.04\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "\n",
            "%Cpu(s):\u001b[m\u001b[m\u001b[1m  0.5 \u001b[m\u001b[mus,\u001b[m\u001b[m\u001b[1m  0.2 \u001b[m\u001b[msy,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mni,\u001b[m\u001b[m\u001b[1m 99.3 \u001b[m\u001b[mid,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mwa,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mhi,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[msi,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mst\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "KiB Mem :\u001b[m\u001b[m\u001b[1m 13335236 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m 10328624 \u001b[m\u001b[mfree,\u001b[m\u001b[m\u001b[1m   412536 \u001b[m\u001b[mused,\u001b[m\u001b[m\u001b[1m  2594076 \u001b[m\u001b[mbuff/cache\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "KiB Swap:\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mfree,\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mused.\u001b[m\u001b[m\u001b[1m 12664976 \u001b[m\u001b[mavail Mem \u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "\u001b[K\n",
            "\n",
            "\u001b[m   62 root      20   0  918032 225100  79496 S   1.0  1.7   0:03.89 python3     \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m   55 root      20   0  186260  57612  12216 S   0.3  0.4   0:01.48 jupyter-no+ \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m    1 root      20   0   39008   6352   5092 S   0.0  0.0   0:00.09 run.sh      \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m    9 root      20   0  677324  50504  25428 S   0.0  0.4   0:00.86 node        \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m   31 root      20   0  680692  46072  25932 S   0.0  0.3   0:00.97 node        \u001b[m\u001b[m\u001b[K\n",
            "\n",
            "\n",
            "\u001b[J\u001b[H\u001b[mtop - 07:05:19 up 11 min,  0 users,  load average: 0.00, 0.04, 0.04\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "\n",
            "%Cpu(s):\u001b[m\u001b[m\u001b[1m  0.7 \u001b[m\u001b[mus,\u001b[m\u001b[m\u001b[1m  0.2 \u001b[m\u001b[msy,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mni,\u001b[m\u001b[m\u001b[1m 99.2 \u001b[m\u001b[mid,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mwa,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mhi,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[msi,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mst\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "KiB Mem :\u001b[m\u001b[m\u001b[1m 13335236 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m 10328784 \u001b[m\u001b[mfree,\u001b[m\u001b[m\u001b[1m   412372 \u001b[m\u001b[mused,\u001b[m\u001b[m\u001b[1m  2594080 \u001b[m\u001b[mbuff/cache\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "KiB Swap:\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mfree,\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mused.\u001b[m\u001b[m\u001b[1m 12665140 \u001b[m\u001b[mavail Mem \u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "\u001b[K\n",
            "\n",
            "\u001b[m   62 root      20   0  918032 225100  79496 S   0.7  1.7   0:03.91 python3     \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m    1 root      20   0   39008   6352   5092 S   0.0  0.0   0:00.09 run.sh      \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m    9 root      20   0  677324  50504  25428 S   0.0  0.4   0:00.86 node        \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m   31 root      20   0  680692  46308  25932 S   0.0  0.3   0:00.97 node        \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m   55 root      20   0  186260  57612  12216 S   0.0  0.4   0:01.48 jupyter-no+ \u001b[m\u001b[m\u001b[K\n",
            "\n",
            "\n",
            "\u001b[J\u001b[H\u001b[mtop - 07:05:22 up 11 min,  0 users,  load average: 0.00, 0.04, 0.04\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "\n",
            "%Cpu(s):\u001b[m\u001b[m\u001b[1m  0.5 \u001b[m\u001b[mus,\u001b[m\u001b[m\u001b[1m  0.2 \u001b[m\u001b[msy,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mni,\u001b[m\u001b[m\u001b[1m 99.3 \u001b[m\u001b[mid,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mwa,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mhi,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[msi,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mst\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "KiB Mem :\u001b[m\u001b[m\u001b[1m 13335236 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m 10328692 \u001b[m\u001b[mfree,\u001b[m\u001b[m\u001b[1m   412456 \u001b[m\u001b[mused,\u001b[m\u001b[m\u001b[1m  2594088 \u001b[m\u001b[mbuff/cache\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "KiB Swap:\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mfree,\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mused.\u001b[m\u001b[m\u001b[1m 12665056 \u001b[m\u001b[mavail Mem \u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "\u001b[K\n",
            "\n",
            "\u001b[m   62 root      20   0  918032 225100  79496 S   0.7  1.7   0:03.93 python3     \u001b[m\u001b[m\u001b[K\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[J\u001b[H\u001b[mtop - 07:05:25 up 11 min,  0 users,  load average: 0.00, 0.04, 0.04\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "\n",
            "\n",
            "KiB Mem :\u001b[m\u001b[m\u001b[1m 13335236 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m 10328632 \u001b[m\u001b[mfree,\u001b[m\u001b[m\u001b[1m   412516 \u001b[m\u001b[mused,\u001b[m\u001b[m\u001b[1m  2594088 \u001b[m\u001b[mbuff/cache\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "KiB Swap:\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mfree,\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mused.\u001b[m\u001b[m\u001b[1m 12665012 \u001b[m\u001b[mavail Mem \u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "\u001b[K\n",
            "\n",
            "\u001b[m   55 root      20   0  186260  57612  12216 S   0.3  0.4   0:01.49 jupyter-no+ \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m   62 root      20   0  918032 225100  79496 S   0.3  1.7   0:03.94 python3     \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m    1 root      20   0   39008   6352   5092 S   0.0  0.0   0:00.09 run.sh      \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m    9 root      20   0  677324  50504  25428 S   0.0  0.4   0:00.86 node        \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m   31 root      20   0  680692  46308  25932 S   0.0  0.3   0:00.97 node        \u001b[m\u001b[m\u001b[K\n",
            "\n",
            "\n",
            "\u001b[J\u001b[H\u001b[mtop - 07:05:28 up 11 min,  0 users,  load average: 0.00, 0.04, 0.04\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "\n",
            "\n",
            "KiB Mem :\u001b[m\u001b[m\u001b[1m 13335236 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m 10328656 \u001b[m\u001b[mfree,\u001b[m\u001b[m\u001b[1m   412488 \u001b[m\u001b[mused,\u001b[m\u001b[m\u001b[1m  2594092 \u001b[m\u001b[mbuff/cache\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "KiB Swap:\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mfree,\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mused.\u001b[m\u001b[m\u001b[1m 12665036 \u001b[m\u001b[mavail Mem \u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "\u001b[K\n",
            "\n",
            "\u001b[m   62 root      20   0  918032 225100  79496 S   1.0  1.7   0:03.97 python3     \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m    1 root      20   0   39008   6352   5092 S   0.0  0.0   0:00.09 run.sh      \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m    9 root      20   0  677324  50504  25428 S   0.0  0.4   0:00.86 node        \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m   31 root      20   0  680692  46308  25932 S   0.0  0.3   0:00.97 node        \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m   55 root      20   0  186260  57628  12216 S   0.0  0.4   0:01.49 jupyter-no+ \u001b[m\u001b[m\u001b[K\n",
            "\n",
            "\n",
            "\u001b[J\u001b[H\u001b[mtop - 07:05:31 up 11 min,  0 users,  load average: 0.00, 0.04, 0.04\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "\n",
            "\n",
            "KiB Mem :\u001b[m\u001b[m\u001b[1m 13335236 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m 10328612 \u001b[m\u001b[mfree,\u001b[m\u001b[m\u001b[1m   412528 \u001b[m\u001b[mused,\u001b[m\u001b[m\u001b[1m  2594096 \u001b[m\u001b[mbuff/cache\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "KiB Swap:\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mfree,\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mused.\u001b[m\u001b[m\u001b[1m 12665004 \u001b[m\u001b[mavail Mem \u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "\u001b[K\n",
            "\n",
            "\u001b[m   62 root      20   0  918032 225100  79496 S   0.7  1.7   0:03.99 python3     \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m   31 root      20   0  680692  46568  25932 S   0.3  0.3   0:00.98 node        \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m    1 root      20   0   39008   6352   5092 S   0.0  0.0   0:00.09 run.sh      \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m    9 root      20   0  677324  50504  25428 S   0.0  0.4   0:00.86 node        \u001b[m\u001b[m\u001b[K\n",
            "\n",
            "\n",
            "\n",
            "\u001b[J\u001b[H\u001b[mtop - 07:05:34 up 11 min,  0 users,  load average: 0.00, 0.04, 0.04\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "\n",
            "%Cpu(s):\u001b[m\u001b[m\u001b[1m  0.7 \u001b[m\u001b[mus,\u001b[m\u001b[m\u001b[1m  0.3 \u001b[m\u001b[msy,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mni,\u001b[m\u001b[m\u001b[1m 99.0 \u001b[m\u001b[mid,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mwa,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mhi,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[msi,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mst\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "KiB Mem :\u001b[m\u001b[m\u001b[1m 13335236 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m 10328644 \u001b[m\u001b[mfree,\u001b[m\u001b[m\u001b[1m   412492 \u001b[m\u001b[mused,\u001b[m\u001b[m\u001b[1m  2594100 \u001b[m\u001b[mbuff/cache\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "KiB Swap:\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mfree,\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mused.\u001b[m\u001b[m\u001b[1m 12665040 \u001b[m\u001b[mavail Mem \u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "\u001b[K\n",
            "\n",
            "\u001b[m   62 root      20   0  918032 225100  79496 S   0.7  1.7   0:04.01 python3     \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m   31 root      20   0  680692  46568  25932 S   0.3  0.3   0:00.99 node        \u001b[m\u001b[m\u001b[K\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[J\u001b[H\u001b[mtop - 07:05:37 up 11 min,  0 users,  load average: 0.00, 0.04, 0.04\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "\n",
            "%Cpu(s):\u001b[m\u001b[m\u001b[1m  0.5 \u001b[m\u001b[mus,\u001b[m\u001b[m\u001b[1m  0.2 \u001b[m\u001b[msy,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mni,\u001b[m\u001b[m\u001b[1m 99.3 \u001b[m\u001b[mid,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mwa,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mhi,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[msi,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mst\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "KiB Mem :\u001b[m\u001b[m\u001b[1m 13335236 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m 10328428 \u001b[m\u001b[mfree,\u001b[m\u001b[m\u001b[1m   412704 \u001b[m\u001b[mused,\u001b[m\u001b[m\u001b[1m  2594104 \u001b[m\u001b[mbuff/cache\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "KiB Swap:\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mfree,\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mused.\u001b[m\u001b[m\u001b[1m 12664836 \u001b[m\u001b[mavail Mem \u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "\u001b[K\n",
            "\n",
            "\u001b[m   31 root      20   0  680692  46568  25932 S   0.3  0.3   0:01.00 node        \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m   55 root      20   0  186260  57628  12216 S   0.3  0.4   0:01.50 jupyter-no+ \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m   62 root      20   0  918032 225100  79496 S   0.3  1.7   0:04.02 python3     \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m\u001b[1m  108 root      20   0   65356   7036   5280 R   0.3  0.1   0:00.02 top         \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m    1 root      20   0   39008   6352   5092 S   0.0  0.0   0:00.09 run.sh      \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m    9 root      20   0  677324  50504  25428 S   0.0  0.4   0:00.86 node        \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m   84 root      20   0   54024  14760   7636 S   0.0  0.1   0:00.08 python3     \u001b[m\u001b[m\u001b[K\n",
            "\u001b[J\u001b[H\u001b[mtop - 07:05:40 up 11 min,  0 users,  load average: 0.00, 0.04, 0.04\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "\n",
            "%Cpu(s):\u001b[m\u001b[m\u001b[1m  0.7 \u001b[m\u001b[mus,\u001b[m\u001b[m\u001b[1m  0.2 \u001b[m\u001b[msy,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mni,\u001b[m\u001b[m\u001b[1m 99.0 \u001b[m\u001b[mid,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mwa,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mhi,\u001b[m\u001b[m\u001b[1m  0.2 \u001b[m\u001b[msi,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mst\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "KiB Mem :\u001b[m\u001b[m\u001b[1m 13335236 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m 10328212 \u001b[m\u001b[mfree,\u001b[m\u001b[m\u001b[1m   412916 \u001b[m\u001b[mused,\u001b[m\u001b[m\u001b[1m  2594108 \u001b[m\u001b[mbuff/cache\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "KiB Swap:\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mfree,\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mused.\u001b[m\u001b[m\u001b[1m 12664624 \u001b[m\u001b[mavail Mem \u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "\u001b[K\n",
            "\n",
            "\u001b[m   62 root      20   0  918032 225100  79496 S   1.0  1.7   0:04.05 python3     \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m    9 root      20   0  677324  50504  25428 S   0.3  0.4   0:00.87 node        \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m    1 root      20   0   39008   6352   5092 S   0.0  0.0   0:00.09 run.sh      \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m   31 root      20   0  680692  46568  25932 S   0.0  0.3   0:01.00 node        \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m   55 root      20   0  186260  57636  12216 S   0.0  0.4   0:01.50 jupyter-no+ \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m   84 root      20   0   54024  14760   7636 S   0.0  0.1   0:00.08 python3     \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m\u001b[1m  108 root      20   0   65356   7036   5280 R   0.0  0.1   0:00.02 top         \u001b[m\u001b[m\u001b[K\n",
            "\u001b[J\u001b[H\u001b[mtop - 07:05:43 up 11 min,  0 users,  load average: 0.00, 0.04, 0.04\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "\n",
            "%Cpu(s):\u001b[m\u001b[m\u001b[1m  0.5 \u001b[m\u001b[mus,\u001b[m\u001b[m\u001b[1m  0.2 \u001b[m\u001b[msy,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mni,\u001b[m\u001b[m\u001b[1m 99.3 \u001b[m\u001b[mid,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mwa,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mhi,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[msi,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mst\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "KiB Mem :\u001b[m\u001b[m\u001b[1m 13335236 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m 10328084 \u001b[m\u001b[mfree,\u001b[m\u001b[m\u001b[1m   413024 \u001b[m\u001b[mused,\u001b[m\u001b[m\u001b[1m  2594128 \u001b[m\u001b[mbuff/cache\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "KiB Swap:\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mfree,\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mused.\u001b[m\u001b[m\u001b[1m 12664524 \u001b[m\u001b[mavail Mem \u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "\u001b[K\n",
            "\n",
            "\u001b[m   62 root      20   0  918032 225100  79496 S   0.3  1.7   0:04.06 python3     \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m    1 root      20   0   39008   6352   5092 S   0.0  0.0   0:00.09 run.sh      \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m    9 root      20   0  677324  50504  25428 S   0.0  0.4   0:00.87 node        \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m   31 root      20   0  680692  46828  25932 S   0.0  0.4   0:01.00 node        \u001b[m\u001b[m\u001b[K\n",
            "\n",
            "\n",
            "\n",
            "\u001b[J\u001b[H\u001b[mtop - 07:05:46 up 11 min,  0 users,  load average: 0.00, 0.04, 0.04\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "\n",
            "\n",
            "KiB Mem :\u001b[m\u001b[m\u001b[1m 13335236 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m 10328080 \u001b[m\u001b[mfree,\u001b[m\u001b[m\u001b[1m   413020 \u001b[m\u001b[mused,\u001b[m\u001b[m\u001b[1m  2594136 \u001b[m\u001b[mbuff/cache\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "KiB Swap:\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mfree,\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mused.\u001b[m\u001b[m\u001b[1m 12664528 \u001b[m\u001b[mavail Mem \u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "\u001b[K\n",
            "\n",
            "\u001b[m   62 root      20   0  918032 225100  79496 S   0.7  1.7   0:04.08 python3     \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m   55 root      20   0  186260  57636  12216 S   0.3  0.4   0:01.51 jupyter-no+ \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m    1 root      20   0   39008   6352   5092 S   0.0  0.0   0:00.09 run.sh      \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m    9 root      20   0  677324  50504  25428 S   0.0  0.4   0:00.87 node        \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m   31 root      20   0  680692  46828  25932 S   0.0  0.4   0:01.00 node        \u001b[m\u001b[m\u001b[K\n",
            "\n",
            "\n",
            "\u001b[J\u001b[?1l\u001b>\u001b[25;1H\n",
            "\u001b[K"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_2AApYA4MZbw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "27c44625-7868-4b58-a59f-4b2024424485"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "x = np.arange(0,10,0.1)\n",
        "y = np.sin(x)\n",
        "plt.plot(x,y)\n",
        "plt.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAAFKCAYAAAAwrQetAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f45adabeeb8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "9IFn9Nv2NpI6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6902
        },
        "outputId": "935ed3f9-b16c-4ff3-a19b-81493c0413db"
      },
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*\n",
        "#https://dev.classmethod.jp/machine-learning/introduction-keras-deeplearning/\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Activation, Dropout, Flatten\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.pooling import MaxPool2D\n",
        "from keras.optimizers import Adam\n",
        " \n",
        "# KerasMNIST\n",
        "# \n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        " \n",
        "# 0-255 -> 0-1\n",
        "X_train = X_train.reshape(60000, 784) / 255\n",
        "X_test = X_test.reshape(10000, 784) / 255\n",
        " \n",
        "# \n",
        "# 0, 1, 235\n",
        "#   [0, 1, 2, 1, 0]\n",
        "# \n",
        "#   [[1, 0, 0],\n",
        "#    [0, 1, 0],\n",
        "#    [0, 0, 1],\n",
        "#    [0, 1, 0],\n",
        "#    [1, 0, 0]]\n",
        "# 0, 1, 210\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        " \n",
        "# \n",
        "# \n",
        "# model.add()\n",
        "model = Sequential([\n",
        "        Dense(512, input_shape=(784,)),\n",
        "        Activation('sigmoid'),\n",
        "        Dense(10),\n",
        "        Activation('softmax')\n",
        "    ])\n",
        "\n",
        "#  \n",
        "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        " \n",
        "# \n",
        "model.fit(X_train, y_train, batch_size=200, verbose=1, epochs=200, validation_split=0.1)\n",
        " \n",
        "# \n",
        "score = model.evaluate(X_test, y_test, verbose=1)\n",
        "print('test accuracy : ', score[1])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n",
            "Train on 54000 samples, validate on 6000 samples\n",
            "Epoch 1/200\n",
            "54000/54000 [==============================] - 3s 63us/step - loss: 2.1005 - acc: 0.4486 - val_loss: 1.8690 - val_acc: 0.6698\n",
            "Epoch 2/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 1.7097 - acc: 0.6970 - val_loss: 1.5082 - val_acc: 0.7798\n",
            "Epoch 3/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 1.4045 - acc: 0.7575 - val_loss: 1.2288 - val_acc: 0.8225\n",
            "Epoch 4/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 1.1759 - acc: 0.7889 - val_loss: 1.0250 - val_acc: 0.8418\n",
            "Epoch 5/200\n",
            "54000/54000 [==============================] - 3s 63us/step - loss: 1.0111 - acc: 0.8084 - val_loss: 0.8797 - val_acc: 0.8595\n",
            "Epoch 6/200\n",
            "54000/54000 [==============================] - 3s 63us/step - loss: 0.8924 - acc: 0.8229 - val_loss: 0.7743 - val_acc: 0.8667\n",
            "Epoch 7/200\n",
            "54000/54000 [==============================] - 4s 67us/step - loss: 0.8048 - acc: 0.8332 - val_loss: 0.6956 - val_acc: 0.8743\n",
            "Epoch 8/200\n",
            "54000/54000 [==============================] - 3s 63us/step - loss: 0.7384 - acc: 0.8408 - val_loss: 0.6360 - val_acc: 0.8785\n",
            "Epoch 9/200\n",
            "54000/54000 [==============================] - 3s 63us/step - loss: 0.6868 - acc: 0.8480 - val_loss: 0.5892 - val_acc: 0.8812\n",
            "Epoch 10/200\n",
            "54000/54000 [==============================] - 3s 63us/step - loss: 0.6455 - acc: 0.8530 - val_loss: 0.5512 - val_acc: 0.8875\n",
            "Epoch 11/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.6119 - acc: 0.8576 - val_loss: 0.5208 - val_acc: 0.8895\n",
            "Epoch 12/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.5840 - acc: 0.8613 - val_loss: 0.4949 - val_acc: 0.8927\n",
            "Epoch 13/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.5603 - acc: 0.8649 - val_loss: 0.4733 - val_acc: 0.8972\n",
            "Epoch 14/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.5401 - acc: 0.8677 - val_loss: 0.4541 - val_acc: 0.8993\n",
            "Epoch 15/200\n",
            "54000/54000 [==============================] - 3s 63us/step - loss: 0.5225 - acc: 0.8702 - val_loss: 0.4383 - val_acc: 0.9005\n",
            "Epoch 16/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.5072 - acc: 0.8727 - val_loss: 0.4242 - val_acc: 0.9013\n",
            "Epoch 17/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.4937 - acc: 0.8749 - val_loss: 0.4124 - val_acc: 0.9037\n",
            "Epoch 18/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.4819 - acc: 0.8773 - val_loss: 0.4008 - val_acc: 0.9040\n",
            "Epoch 19/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.4711 - acc: 0.8788 - val_loss: 0.3913 - val_acc: 0.9055\n",
            "Epoch 20/200\n",
            "54000/54000 [==============================] - 3s 63us/step - loss: 0.4614 - acc: 0.8805 - val_loss: 0.3827 - val_acc: 0.9068\n",
            "Epoch 21/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.4526 - acc: 0.8822 - val_loss: 0.3753 - val_acc: 0.9063\n",
            "Epoch 22/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.4448 - acc: 0.8829 - val_loss: 0.3678 - val_acc: 0.9088\n",
            "Epoch 23/200\n",
            "54000/54000 [==============================] - 3s 64us/step - loss: 0.4375 - acc: 0.8842 - val_loss: 0.3612 - val_acc: 0.9090\n",
            "Epoch 24/200\n",
            "54000/54000 [==============================] - 3s 63us/step - loss: 0.4309 - acc: 0.8858 - val_loss: 0.3554 - val_acc: 0.9115\n",
            "Epoch 25/200\n",
            "54000/54000 [==============================] - 3s 64us/step - loss: 0.4249 - acc: 0.8861 - val_loss: 0.3500 - val_acc: 0.9110\n",
            "Epoch 26/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.4193 - acc: 0.8871 - val_loss: 0.3450 - val_acc: 0.9108\n",
            "Epoch 27/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.4140 - acc: 0.8883 - val_loss: 0.3410 - val_acc: 0.9103\n",
            "Epoch 28/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.4091 - acc: 0.8891 - val_loss: 0.3363 - val_acc: 0.9122\n",
            "Epoch 29/200\n",
            "54000/54000 [==============================] - 3s 63us/step - loss: 0.4046 - acc: 0.8898 - val_loss: 0.3322 - val_acc: 0.9125\n",
            "Epoch 30/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.4003 - acc: 0.8900 - val_loss: 0.3289 - val_acc: 0.9130\n",
            "Epoch 31/200\n",
            "54000/54000 [==============================] - 3s 60us/step - loss: 0.3964 - acc: 0.8911 - val_loss: 0.3257 - val_acc: 0.9145\n",
            "Epoch 32/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.3927 - acc: 0.8921 - val_loss: 0.3221 - val_acc: 0.9143\n",
            "Epoch 33/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.3893 - acc: 0.8929 - val_loss: 0.3190 - val_acc: 0.9140\n",
            "Epoch 34/200\n",
            "54000/54000 [==============================] - 3s 64us/step - loss: 0.3858 - acc: 0.8930 - val_loss: 0.3163 - val_acc: 0.9137\n",
            "Epoch 35/200\n",
            "54000/54000 [==============================] - 4s 66us/step - loss: 0.3827 - acc: 0.8941 - val_loss: 0.3138 - val_acc: 0.9163\n",
            "Epoch 36/200\n",
            "54000/54000 [==============================] - 4s 67us/step - loss: 0.3798 - acc: 0.8940 - val_loss: 0.3115 - val_acc: 0.9167\n",
            "Epoch 37/200\n",
            "54000/54000 [==============================] - 3s 63us/step - loss: 0.3770 - acc: 0.8948 - val_loss: 0.3092 - val_acc: 0.9148\n",
            "Epoch 38/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.3743 - acc: 0.8952 - val_loss: 0.3065 - val_acc: 0.9167\n",
            "Epoch 39/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.3718 - acc: 0.8959 - val_loss: 0.3044 - val_acc: 0.9165\n",
            "Epoch 40/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.3693 - acc: 0.8965 - val_loss: 0.3032 - val_acc: 0.9162\n",
            "Epoch 41/200\n",
            "54000/54000 [==============================] - 3s 64us/step - loss: 0.3670 - acc: 0.8970 - val_loss: 0.3009 - val_acc: 0.9168\n",
            "Epoch 42/200\n",
            "54000/54000 [==============================] - 3s 60us/step - loss: 0.3648 - acc: 0.8975 - val_loss: 0.2986 - val_acc: 0.9185\n",
            "Epoch 43/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.3628 - acc: 0.8976 - val_loss: 0.2968 - val_acc: 0.9183\n",
            "Epoch 44/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.3607 - acc: 0.8984 - val_loss: 0.2953 - val_acc: 0.9183\n",
            "Epoch 45/200\n",
            "54000/54000 [==============================] - 3s 60us/step - loss: 0.3588 - acc: 0.8989 - val_loss: 0.2938 - val_acc: 0.9182\n",
            "Epoch 46/200\n",
            "54000/54000 [==============================] - 3s 60us/step - loss: 0.3569 - acc: 0.8994 - val_loss: 0.2919 - val_acc: 0.9192\n",
            "Epoch 47/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.3550 - acc: 0.8995 - val_loss: 0.2906 - val_acc: 0.9198\n",
            "Epoch 48/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.3534 - acc: 0.8997 - val_loss: 0.2893 - val_acc: 0.9197\n",
            "Epoch 49/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.3517 - acc: 0.9002 - val_loss: 0.2878 - val_acc: 0.9197\n",
            "Epoch 50/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.3501 - acc: 0.9008 - val_loss: 0.2864 - val_acc: 0.9203\n",
            "Epoch 51/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.3485 - acc: 0.9012 - val_loss: 0.2858 - val_acc: 0.9198\n",
            "Epoch 52/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.3471 - acc: 0.9014 - val_loss: 0.2840 - val_acc: 0.9207\n",
            "Epoch 53/200\n",
            "54000/54000 [==============================] - 3s 63us/step - loss: 0.3456 - acc: 0.9019 - val_loss: 0.2828 - val_acc: 0.9212\n",
            "Epoch 54/200\n",
            "54000/54000 [==============================] - 3s 64us/step - loss: 0.3441 - acc: 0.9021 - val_loss: 0.2821 - val_acc: 0.9200\n",
            "Epoch 55/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.3428 - acc: 0.9026 - val_loss: 0.2807 - val_acc: 0.9212\n",
            "Epoch 56/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.3415 - acc: 0.9026 - val_loss: 0.2797 - val_acc: 0.9207\n",
            "Epoch 57/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.3402 - acc: 0.9034 - val_loss: 0.2787 - val_acc: 0.9223\n",
            "Epoch 58/200\n",
            "54000/54000 [==============================] - 3s 60us/step - loss: 0.3390 - acc: 0.9034 - val_loss: 0.2775 - val_acc: 0.9225\n",
            "Epoch 59/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.3378 - acc: 0.9041 - val_loss: 0.2768 - val_acc: 0.9222\n",
            "Epoch 60/200\n",
            "54000/54000 [==============================] - 3s 60us/step - loss: 0.3367 - acc: 0.9036 - val_loss: 0.2760 - val_acc: 0.9212\n",
            "Epoch 61/200\n",
            "54000/54000 [==============================] - 3s 60us/step - loss: 0.3355 - acc: 0.9044 - val_loss: 0.2751 - val_acc: 0.9230\n",
            "Epoch 62/200\n",
            "54000/54000 [==============================] - 3s 60us/step - loss: 0.3344 - acc: 0.9045 - val_loss: 0.2743 - val_acc: 0.9230\n",
            "Epoch 63/200\n",
            "54000/54000 [==============================] - 3s 60us/step - loss: 0.3333 - acc: 0.9048 - val_loss: 0.2735 - val_acc: 0.9220\n",
            "Epoch 64/200\n",
            "54000/54000 [==============================] - 3s 60us/step - loss: 0.3323 - acc: 0.9049 - val_loss: 0.2725 - val_acc: 0.9230\n",
            "Epoch 65/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.3313 - acc: 0.9053 - val_loss: 0.2716 - val_acc: 0.9238\n",
            "Epoch 66/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.3302 - acc: 0.9055 - val_loss: 0.2712 - val_acc: 0.9223\n",
            "Epoch 67/200\n",
            "54000/54000 [==============================] - 3s 60us/step - loss: 0.3292 - acc: 0.9061 - val_loss: 0.2700 - val_acc: 0.9235\n",
            "Epoch 68/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.3284 - acc: 0.9061 - val_loss: 0.2693 - val_acc: 0.9243\n",
            "Epoch 69/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.3275 - acc: 0.9060 - val_loss: 0.2688 - val_acc: 0.9245\n",
            "Epoch 70/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.3266 - acc: 0.9065 - val_loss: 0.2681 - val_acc: 0.9245\n",
            "Epoch 71/200\n",
            "54000/54000 [==============================] - 3s 60us/step - loss: 0.3256 - acc: 0.9069 - val_loss: 0.2671 - val_acc: 0.9253\n",
            "Epoch 72/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.3248 - acc: 0.9074 - val_loss: 0.2667 - val_acc: 0.9242\n",
            "Epoch 73/200\n",
            "54000/54000 [==============================] - 3s 60us/step - loss: 0.3240 - acc: 0.9071 - val_loss: 0.2663 - val_acc: 0.9250\n",
            "Epoch 74/200\n",
            "54000/54000 [==============================] - 3s 60us/step - loss: 0.3231 - acc: 0.9073 - val_loss: 0.2652 - val_acc: 0.9257\n",
            "Epoch 75/200\n",
            "54000/54000 [==============================] - 3s 60us/step - loss: 0.3224 - acc: 0.9074 - val_loss: 0.2650 - val_acc: 0.9252\n",
            "Epoch 76/200\n",
            "54000/54000 [==============================] - 3s 60us/step - loss: 0.3215 - acc: 0.9079 - val_loss: 0.2641 - val_acc: 0.9248\n",
            "Epoch 77/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.3208 - acc: 0.9082 - val_loss: 0.2636 - val_acc: 0.9250\n",
            "Epoch 78/200\n",
            "54000/54000 [==============================] - 3s 60us/step - loss: 0.3201 - acc: 0.9084 - val_loss: 0.2630 - val_acc: 0.9248\n",
            "Epoch 79/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.3193 - acc: 0.9085 - val_loss: 0.2625 - val_acc: 0.9257\n",
            "Epoch 80/200\n",
            "54000/54000 [==============================] - 3s 60us/step - loss: 0.3186 - acc: 0.9086 - val_loss: 0.2623 - val_acc: 0.9252\n",
            "Epoch 81/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.3179 - acc: 0.9091 - val_loss: 0.2613 - val_acc: 0.9258\n",
            "Epoch 82/200\n",
            "54000/54000 [==============================] - 3s 60us/step - loss: 0.3172 - acc: 0.9090 - val_loss: 0.2613 - val_acc: 0.9252\n",
            "Epoch 83/200\n",
            "54000/54000 [==============================] - 3s 60us/step - loss: 0.3165 - acc: 0.9091 - val_loss: 0.2603 - val_acc: 0.9263\n",
            "Epoch 84/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.3158 - acc: 0.9092 - val_loss: 0.2596 - val_acc: 0.9270\n",
            "Epoch 85/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.3151 - acc: 0.9096 - val_loss: 0.2593 - val_acc: 0.9262\n",
            "Epoch 86/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.3145 - acc: 0.9098 - val_loss: 0.2589 - val_acc: 0.9268\n",
            "Epoch 87/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.3139 - acc: 0.9101 - val_loss: 0.2586 - val_acc: 0.9268\n",
            "Epoch 88/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.3132 - acc: 0.9101 - val_loss: 0.2578 - val_acc: 0.9270\n",
            "Epoch 89/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.3126 - acc: 0.9106 - val_loss: 0.2575 - val_acc: 0.9282\n",
            "Epoch 90/200\n",
            "54000/54000 [==============================] - 3s 63us/step - loss: 0.3120 - acc: 0.9107 - val_loss: 0.2567 - val_acc: 0.9277\n",
            "Epoch 91/200\n",
            "54000/54000 [==============================] - 3s 63us/step - loss: 0.3115 - acc: 0.9106 - val_loss: 0.2563 - val_acc: 0.9273\n",
            "Epoch 92/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.3109 - acc: 0.9109 - val_loss: 0.2559 - val_acc: 0.9270\n",
            "Epoch 93/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.3102 - acc: 0.9114 - val_loss: 0.2561 - val_acc: 0.9280\n",
            "Epoch 94/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.3096 - acc: 0.9113 - val_loss: 0.2557 - val_acc: 0.9277\n",
            "Epoch 95/200\n",
            "54000/54000 [==============================] - 3s 64us/step - loss: 0.3091 - acc: 0.9119 - val_loss: 0.2547 - val_acc: 0.9290\n",
            "Epoch 96/200\n",
            "54000/54000 [==============================] - 3s 64us/step - loss: 0.3086 - acc: 0.9118 - val_loss: 0.2543 - val_acc: 0.9280\n",
            "Epoch 97/200\n",
            "54000/54000 [==============================] - 3s 63us/step - loss: 0.3080 - acc: 0.9122 - val_loss: 0.2542 - val_acc: 0.9282\n",
            "Epoch 98/200\n",
            "54000/54000 [==============================] - 3s 63us/step - loss: 0.3075 - acc: 0.9124 - val_loss: 0.2538 - val_acc: 0.9278\n",
            "Epoch 99/200\n",
            "54000/54000 [==============================] - 3s 63us/step - loss: 0.3070 - acc: 0.9124 - val_loss: 0.2534 - val_acc: 0.9277\n",
            "Epoch 100/200\n",
            "54000/54000 [==============================] - 3s 63us/step - loss: 0.3063 - acc: 0.9124 - val_loss: 0.2529 - val_acc: 0.9282\n",
            "Epoch 101/200\n",
            "54000/54000 [==============================] - 3s 64us/step - loss: 0.3059 - acc: 0.9129 - val_loss: 0.2525 - val_acc: 0.9275\n",
            "Epoch 102/200\n",
            "54000/54000 [==============================] - 3s 63us/step - loss: 0.3054 - acc: 0.9130 - val_loss: 0.2518 - val_acc: 0.9287\n",
            "Epoch 103/200\n",
            "54000/54000 [==============================] - 3s 63us/step - loss: 0.3050 - acc: 0.9132 - val_loss: 0.2515 - val_acc: 0.9282\n",
            "Epoch 104/200\n",
            "54000/54000 [==============================] - 3s 64us/step - loss: 0.3045 - acc: 0.9129 - val_loss: 0.2515 - val_acc: 0.9285\n",
            "Epoch 105/200\n",
            "54000/54000 [==============================] - 3s 63us/step - loss: 0.3040 - acc: 0.9132 - val_loss: 0.2510 - val_acc: 0.9295\n",
            "Epoch 106/200\n",
            "54000/54000 [==============================] - 3s 64us/step - loss: 0.3034 - acc: 0.9133 - val_loss: 0.2504 - val_acc: 0.9287\n",
            "Epoch 107/200\n",
            "54000/54000 [==============================] - 3s 64us/step - loss: 0.3030 - acc: 0.9136 - val_loss: 0.2503 - val_acc: 0.9288\n",
            "Epoch 108/200\n",
            "54000/54000 [==============================] - 3s 64us/step - loss: 0.3025 - acc: 0.9138 - val_loss: 0.2500 - val_acc: 0.9290\n",
            "Epoch 109/200\n",
            "54000/54000 [==============================] - 3s 64us/step - loss: 0.3020 - acc: 0.9138 - val_loss: 0.2498 - val_acc: 0.9287\n",
            "Epoch 110/200\n",
            "54000/54000 [==============================] - 3s 63us/step - loss: 0.3016 - acc: 0.9140 - val_loss: 0.2492 - val_acc: 0.9293\n",
            "Epoch 111/200\n",
            "54000/54000 [==============================] - 3s 59us/step - loss: 0.3011 - acc: 0.9140 - val_loss: 0.2493 - val_acc: 0.9285\n",
            "Epoch 112/200\n",
            "54000/54000 [==============================] - 3s 59us/step - loss: 0.3007 - acc: 0.9142 - val_loss: 0.2491 - val_acc: 0.9282\n",
            "Epoch 113/200\n",
            "54000/54000 [==============================] - 3s 60us/step - loss: 0.3003 - acc: 0.9140 - val_loss: 0.2484 - val_acc: 0.9293\n",
            "Epoch 114/200\n",
            "54000/54000 [==============================] - 3s 59us/step - loss: 0.2997 - acc: 0.9144 - val_loss: 0.2479 - val_acc: 0.9285\n",
            "Epoch 115/200\n",
            "54000/54000 [==============================] - 3s 60us/step - loss: 0.2994 - acc: 0.9146 - val_loss: 0.2477 - val_acc: 0.9292\n",
            "Epoch 116/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.2989 - acc: 0.9147 - val_loss: 0.2473 - val_acc: 0.9295\n",
            "Epoch 117/200\n",
            "54000/54000 [==============================] - 3s 60us/step - loss: 0.2984 - acc: 0.9151 - val_loss: 0.2473 - val_acc: 0.9290\n",
            "Epoch 118/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.2981 - acc: 0.9149 - val_loss: 0.2465 - val_acc: 0.9293\n",
            "Epoch 119/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.2976 - acc: 0.9150 - val_loss: 0.2464 - val_acc: 0.9288\n",
            "Epoch 120/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.2973 - acc: 0.9156 - val_loss: 0.2460 - val_acc: 0.9292\n",
            "Epoch 121/200\n",
            "54000/54000 [==============================] - 3s 60us/step - loss: 0.2969 - acc: 0.9154 - val_loss: 0.2458 - val_acc: 0.9290\n",
            "Epoch 122/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.2964 - acc: 0.9154 - val_loss: 0.2453 - val_acc: 0.9303\n",
            "Epoch 123/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.2960 - acc: 0.9156 - val_loss: 0.2449 - val_acc: 0.9293\n",
            "Epoch 124/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.2957 - acc: 0.9156 - val_loss: 0.2447 - val_acc: 0.9293\n",
            "Epoch 125/200\n",
            "54000/54000 [==============================] - 3s 60us/step - loss: 0.2953 - acc: 0.9155 - val_loss: 0.2446 - val_acc: 0.9292\n",
            "Epoch 126/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.2949 - acc: 0.9161 - val_loss: 0.2442 - val_acc: 0.9283\n",
            "Epoch 127/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.2945 - acc: 0.9161 - val_loss: 0.2440 - val_acc: 0.9292\n",
            "Epoch 128/200\n",
            "54000/54000 [==============================] - 4s 66us/step - loss: 0.2941 - acc: 0.9157 - val_loss: 0.2438 - val_acc: 0.9302\n",
            "Epoch 129/200\n",
            "54000/54000 [==============================] - 4s 65us/step - loss: 0.2938 - acc: 0.9163 - val_loss: 0.2435 - val_acc: 0.9303\n",
            "Epoch 130/200\n",
            "54000/54000 [==============================] - 3s 65us/step - loss: 0.2933 - acc: 0.9167 - val_loss: 0.2431 - val_acc: 0.9300\n",
            "Epoch 131/200\n",
            "54000/54000 [==============================] - 3s 64us/step - loss: 0.2930 - acc: 0.9161 - val_loss: 0.2430 - val_acc: 0.9308\n",
            "Epoch 132/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.2926 - acc: 0.9166 - val_loss: 0.2426 - val_acc: 0.9292\n",
            "Epoch 133/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.2923 - acc: 0.9161 - val_loss: 0.2427 - val_acc: 0.9302\n",
            "Epoch 134/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.2918 - acc: 0.9170 - val_loss: 0.2418 - val_acc: 0.9297\n",
            "Epoch 135/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.2914 - acc: 0.9164 - val_loss: 0.2417 - val_acc: 0.9297\n",
            "Epoch 136/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.2912 - acc: 0.9167 - val_loss: 0.2417 - val_acc: 0.9295\n",
            "Epoch 137/200\n",
            "54000/54000 [==============================] - 3s 63us/step - loss: 0.2908 - acc: 0.9174 - val_loss: 0.2413 - val_acc: 0.9300\n",
            "Epoch 138/200\n",
            "54000/54000 [==============================] - 3s 63us/step - loss: 0.2904 - acc: 0.9173 - val_loss: 0.2409 - val_acc: 0.9302\n",
            "Epoch 139/200\n",
            "54000/54000 [==============================] - 3s 63us/step - loss: 0.2901 - acc: 0.9172 - val_loss: 0.2412 - val_acc: 0.9305\n",
            "Epoch 140/200\n",
            "54000/54000 [==============================] - 3s 64us/step - loss: 0.2898 - acc: 0.9176 - val_loss: 0.2406 - val_acc: 0.9302\n",
            "Epoch 141/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.2894 - acc: 0.9172 - val_loss: 0.2405 - val_acc: 0.9297\n",
            "Epoch 142/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.2891 - acc: 0.9172 - val_loss: 0.2401 - val_acc: 0.9308\n",
            "Epoch 143/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.2888 - acc: 0.9172 - val_loss: 0.2400 - val_acc: 0.9295\n",
            "Epoch 144/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.2884 - acc: 0.9175 - val_loss: 0.2395 - val_acc: 0.9308\n",
            "Epoch 145/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.2880 - acc: 0.9177 - val_loss: 0.2391 - val_acc: 0.9308\n",
            "Epoch 146/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.2877 - acc: 0.9177 - val_loss: 0.2391 - val_acc: 0.9302\n",
            "Epoch 147/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.2874 - acc: 0.9181 - val_loss: 0.2386 - val_acc: 0.9305\n",
            "Epoch 148/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.2871 - acc: 0.9182 - val_loss: 0.2386 - val_acc: 0.9313\n",
            "Epoch 149/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.2867 - acc: 0.9183 - val_loss: 0.2383 - val_acc: 0.9312\n",
            "Epoch 150/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.2864 - acc: 0.9183 - val_loss: 0.2380 - val_acc: 0.9317\n",
            "Epoch 151/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.2860 - acc: 0.9186 - val_loss: 0.2379 - val_acc: 0.9315\n",
            "Epoch 152/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.2858 - acc: 0.9182 - val_loss: 0.2376 - val_acc: 0.9310\n",
            "Epoch 153/200\n",
            "54000/54000 [==============================] - 3s 63us/step - loss: 0.2854 - acc: 0.9185 - val_loss: 0.2376 - val_acc: 0.9305\n",
            "Epoch 154/200\n",
            "54000/54000 [==============================] - 3s 60us/step - loss: 0.2850 - acc: 0.9184 - val_loss: 0.2370 - val_acc: 0.9315\n",
            "Epoch 155/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.2848 - acc: 0.9188 - val_loss: 0.2368 - val_acc: 0.9315\n",
            "Epoch 156/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.2844 - acc: 0.9187 - val_loss: 0.2365 - val_acc: 0.9312\n",
            "Epoch 157/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.2841 - acc: 0.9188 - val_loss: 0.2364 - val_acc: 0.9315\n",
            "Epoch 158/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.2838 - acc: 0.9191 - val_loss: 0.2362 - val_acc: 0.9312\n",
            "Epoch 159/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.2835 - acc: 0.9186 - val_loss: 0.2360 - val_acc: 0.9320\n",
            "Epoch 160/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.2832 - acc: 0.9189 - val_loss: 0.2358 - val_acc: 0.9322\n",
            "Epoch 161/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.2828 - acc: 0.9188 - val_loss: 0.2355 - val_acc: 0.9315\n",
            "Epoch 162/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.2826 - acc: 0.9193 - val_loss: 0.2354 - val_acc: 0.9317\n",
            "Epoch 163/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.2824 - acc: 0.9194 - val_loss: 0.2352 - val_acc: 0.9323\n",
            "Epoch 164/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.2820 - acc: 0.9196 - val_loss: 0.2349 - val_acc: 0.9320\n",
            "Epoch 165/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.2817 - acc: 0.9195 - val_loss: 0.2346 - val_acc: 0.9318\n",
            "Epoch 166/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.2814 - acc: 0.9196 - val_loss: 0.2342 - val_acc: 0.9323\n",
            "Epoch 167/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.2810 - acc: 0.9194 - val_loss: 0.2340 - val_acc: 0.9327\n",
            "Epoch 168/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.2807 - acc: 0.9200 - val_loss: 0.2345 - val_acc: 0.9328\n",
            "Epoch 169/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.2805 - acc: 0.9197 - val_loss: 0.2336 - val_acc: 0.9320\n",
            "Epoch 170/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.2802 - acc: 0.9199 - val_loss: 0.2334 - val_acc: 0.9328\n",
            "Epoch 171/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.2798 - acc: 0.9201 - val_loss: 0.2331 - val_acc: 0.9328\n",
            "Epoch 172/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.2796 - acc: 0.9204 - val_loss: 0.2330 - val_acc: 0.9328\n",
            "Epoch 173/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.2792 - acc: 0.9202 - val_loss: 0.2328 - val_acc: 0.9320\n",
            "Epoch 174/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.2790 - acc: 0.9205 - val_loss: 0.2326 - val_acc: 0.9332\n",
            "Epoch 175/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.2786 - acc: 0.9204 - val_loss: 0.2322 - val_acc: 0.9325\n",
            "Epoch 176/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.2784 - acc: 0.9205 - val_loss: 0.2324 - val_acc: 0.9332\n",
            "Epoch 177/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.2780 - acc: 0.9209 - val_loss: 0.2323 - val_acc: 0.9322\n",
            "Epoch 178/200\n",
            "54000/54000 [==============================] - 3s 63us/step - loss: 0.2777 - acc: 0.9210 - val_loss: 0.2319 - val_acc: 0.9328\n",
            "Epoch 179/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.2775 - acc: 0.9209 - val_loss: 0.2315 - val_acc: 0.9328\n",
            "Epoch 180/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.2772 - acc: 0.9209 - val_loss: 0.2312 - val_acc: 0.9325\n",
            "Epoch 181/200\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.2769 - acc: 0.9210 - val_loss: 0.2311 - val_acc: 0.9332\n",
            "Epoch 182/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.2767 - acc: 0.9215 - val_loss: 0.2310 - val_acc: 0.9338\n",
            "Epoch 183/200\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.2763 - acc: 0.9212 - val_loss: 0.2306 - val_acc: 0.9325\n",
            "Epoch 184/200\n",
            "54000/54000 [==============================] - 3s 63us/step - loss: 0.2760 - acc: 0.9212 - val_loss: 0.2306 - val_acc: 0.9345\n",
            "Epoch 185/200\n",
            "54000/54000 [==============================] - 3s 64us/step - loss: 0.2757 - acc: 0.9216 - val_loss: 0.2304 - val_acc: 0.9345\n",
            "Epoch 186/200\n",
            "54000/54000 [==============================] - 3s 64us/step - loss: 0.2755 - acc: 0.9215 - val_loss: 0.2299 - val_acc: 0.9332\n",
            "Epoch 187/200\n",
            "54000/54000 [==============================] - 3s 63us/step - loss: 0.2751 - acc: 0.9219 - val_loss: 0.2304 - val_acc: 0.9348\n",
            "Epoch 188/200\n",
            "54000/54000 [==============================] - 3s 64us/step - loss: 0.2748 - acc: 0.9218 - val_loss: 0.2295 - val_acc: 0.9342\n",
            "Epoch 189/200\n",
            "54000/54000 [==============================] - 3s 64us/step - loss: 0.2745 - acc: 0.9220 - val_loss: 0.2292 - val_acc: 0.9345\n",
            "Epoch 190/200\n",
            "54000/54000 [==============================] - 3s 64us/step - loss: 0.2743 - acc: 0.9223 - val_loss: 0.2290 - val_acc: 0.9337\n",
            "Epoch 191/200\n",
            "54000/54000 [==============================] - 3s 65us/step - loss: 0.2741 - acc: 0.9221 - val_loss: 0.2291 - val_acc: 0.9335\n",
            "Epoch 192/200\n",
            "54000/54000 [==============================] - 3s 65us/step - loss: 0.2737 - acc: 0.9218 - val_loss: 0.2286 - val_acc: 0.9332\n",
            "Epoch 193/200\n",
            "54000/54000 [==============================] - 4s 67us/step - loss: 0.2735 - acc: 0.9222 - val_loss: 0.2284 - val_acc: 0.9343\n",
            "Epoch 194/200\n",
            "54000/54000 [==============================] - 4s 66us/step - loss: 0.2732 - acc: 0.9224 - val_loss: 0.2284 - val_acc: 0.9333\n",
            "Epoch 195/200\n",
            "54000/54000 [==============================] - 4s 67us/step - loss: 0.2729 - acc: 0.9226 - val_loss: 0.2282 - val_acc: 0.9342\n",
            "Epoch 196/200\n",
            "54000/54000 [==============================] - 4s 66us/step - loss: 0.2726 - acc: 0.9224 - val_loss: 0.2281 - val_acc: 0.9355\n",
            "Epoch 197/200\n",
            "54000/54000 [==============================] - 4s 66us/step - loss: 0.2723 - acc: 0.9229 - val_loss: 0.2276 - val_acc: 0.9350\n",
            "Epoch 198/200\n",
            "54000/54000 [==============================] - 4s 66us/step - loss: 0.2720 - acc: 0.9227 - val_loss: 0.2275 - val_acc: 0.9350\n",
            "Epoch 199/200\n",
            "54000/54000 [==============================] - 4s 65us/step - loss: 0.2718 - acc: 0.9231 - val_loss: 0.2272 - val_acc: 0.9352\n",
            "Epoch 200/200\n",
            "54000/54000 [==============================] - 4s 66us/step - loss: 0.2715 - acc: 0.9228 - val_loss: 0.2271 - val_acc: 0.9362\n",
            "10000/10000 [==============================] - 1s 61us/step\n",
            "test accuracy :  0.9248\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4OJ6zPkBONhh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "136c3824-7b03-4ab3-f7d8-61516c9ab9e4"
      },
      "cell_type": "code",
      "source": [
        "!df -h"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filesystem      Size  Used Avail Use% Mounted on\n",
            "overlay          40G  7.7G   30G  21% /\n",
            "tmpfs           6.4G     0  6.4G   0% /dev\n",
            "tmpfs           6.4G     0  6.4G   0% /sys/fs/cgroup\n",
            "tmpfs           6.4G     0  6.4G   0% /var/colab\n",
            "/dev/sda1        46G  8.5G   37G  19% /etc/hosts\n",
            "shm              64M     0   64M   0% /dev/shm\n",
            "tmpfs           6.4G     0  6.4G   0% /sys/firmware\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "e_wiMuRwR9fw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!who"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dRZS-YIASEtH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "51ea38ed-ee05-44c1-8bc4-ce3333f1d9aa"
      },
      "cell_type": "code",
      "source": [
        "!which awk"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/bin/awk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "l-kRCJJsSJnx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "46534bc0-3f12-4e0b-af72-910946bc204b"
      },
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*\n",
        "#https://qiita.com/fukuit/items/b3fa460577a0ea139c88\n",
        "'''\n",
        "Keras(+Tensorflow)MNIST\n",
        "\n",
        "'''\n",
        "\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.callbacks import Callback, CSVLogger\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import argparse\n",
        "\n",
        "\n",
        "class PlotLosses(Callback):\n",
        "    '''\n",
        "    losslive plot\n",
        "    '''\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        '''\n",
        "        \n",
        "        '''\n",
        "        self.epoch_cnt = 0      # epoch\n",
        "        plt.axis([0, self.epochs, 0, 0.25])\n",
        "        plt.ion()               # pyplotinteractive mode\n",
        "\n",
        "    def on_train_end(self, logs={}):\n",
        "        '''\n",
        "        \n",
        "        '''\n",
        "        plt.ioff()              # pyplotinteractive modeoff\n",
        "        plt.legend(['loss', 'val_loss'], loc='best')\n",
        "        plt.show()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        '''\n",
        "        epoch\n",
        "        '''\n",
        "        loss = logs.get('loss')\n",
        "        val_loss = logs.get('val_loss')\n",
        "        x = self.epoch_cnt\n",
        "        # epochlossval_lossplot\n",
        "        plt.scatter(x, loss, c='b', label='loss')\n",
        "        plt.scatter(x, val_loss, c='r', label='val_loss')\n",
        "        plt.pause(0.05)\n",
        "        # epochcount up\n",
        "        self.epoch_cnt += 1\n",
        "\n",
        "\n",
        "def plot_result(history):\n",
        "    '''\n",
        "    plot result\n",
        "    historyaccuracylossplot\n",
        "    '''\n",
        "\n",
        "    # accuracy\n",
        "    plt.figure()\n",
        "    plt.plot(history.history['acc'], label='acc', marker='.')\n",
        "    plt.plot(history.history['val_acc'], label='val_acc', marker='.')\n",
        "    plt.grid()\n",
        "    plt.legend(loc='best')\n",
        "    plt.title('accuracy')\n",
        "    plt.savefig('graph_accuracy.png')\n",
        "    plt.show()\n",
        "\n",
        "    # loss\n",
        "    plt.figure()\n",
        "    plt.plot(history.history['loss'], label='loss', marker='.')\n",
        "    plt.plot(history.history['val_loss'], label='val_loss', marker='.')\n",
        "    plt.grid()\n",
        "    plt.legend(loc='best')\n",
        "    plt.title('loss')\n",
        "    plt.savefig('graph_loss.png')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def main(epochs=5, batch_size=128):\n",
        "    '''\n",
        "    MNIST\n",
        "    @args:\n",
        "        epochs: epoch\n",
        "        batch_size: \n",
        "    '''\n",
        "\n",
        "    # load MNIST data\n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "    x_train1, x_valid, y_train1, y_valid = train_test_split(x_train, y_train, test_size=0.175)\n",
        "    x_train = x_train1\n",
        "    y_train = y_train1\n",
        "\n",
        "    x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32')/255\n",
        "    x_valid = x_valid.reshape(x_valid.shape[0], 28, 28, 1).astype('float32')/255\n",
        "    x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float32')/255\n",
        "\n",
        "    # convert one-hot vector\n",
        "    y_train = keras.utils.to_categorical(y_train, 10)\n",
        "    y_valid = keras.utils.to_categorical(y_valid, 10)\n",
        "    y_test = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=RMSprop(),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    print(model.summary())\n",
        "\n",
        "    # callback function\n",
        "    plot_losses = PlotLosses()      # (live plot)\n",
        "    plot_losses.epochs = epochs\n",
        "    csv_logger = CSVLogger('trainlog.csv')\n",
        "\n",
        "    # train\n",
        "    history = model.fit(x_train, y_train,\n",
        "                        batch_size=batch_size, epochs=epochs,\n",
        "                        verbose=1,\n",
        "                        validation_data=(x_valid, y_valid),\n",
        "                        callbacks=[plot_losses, csv_logger])\n",
        "\n",
        "    # result\n",
        "    score = model.evaluate(x_test, y_test, verbose=0)\n",
        "    print('Test loss: {0}'.format(score[0]))\n",
        "    print('Test accuracy: {0}'.format(score[1]))\n",
        "\n",
        "    plot_result(history)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(description='MNIST')\n",
        "    parser.add_argument('--epochs', dest='epochs', type=int, help='size of epochs')\n",
        "    parser.add_argument('--batch_size', dest='batch_size', type=int, help='size of batch')\n",
        "    args = parser.parse_args()\n",
        "    if args.epochs:\n",
        "        epochs = args.epochs\n",
        "    else:\n",
        "        epochs = 100\n",
        "    if args.batch_size:\n",
        "        batch_size = args.batch_size\n",
        "    else:\n",
        "        batch_size = 128\n",
        "\n",
        "    main(epochs, batch_size)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: ipykernel_launcher.py [-h] [--epochs EPOCHS] [--batch_size BATCH_SIZE]\n",
            "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-2504fed7-3ec6-45e3-afda-60d69d8c6d8c.json\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "_hqVzbBH0kx6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "08b1aead-a912-42f5-bf9e-ba2acc1a43a9"
      },
      "cell_type": "code",
      "source": [
        "import caffe"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-6e7bb19bc708>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcaffe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'caffe'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "jzKot0Jv1Qd5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import cv2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mu4gmSNX1ZKR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#  --- (*1)\n",
        "iris_data = pd.read_csv(\"iris.csv\", encoding=\"utf-8\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C3dfJQ4o18Tc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1969
        },
        "outputId": "cbb6fc60-ba5a-47d4-d715-f65c027c6238"
      },
      "cell_type": "code",
      "source": [
        "iris_data"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SepalLength</th>\n",
              "      <th>SepalWidth</th>\n",
              "      <th>PetalLength</th>\n",
              "      <th>PetalWidth</th>\n",
              "      <th>Name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5.4</td>\n",
              "      <td>3.9</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.4</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.3</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>4.4</td>\n",
              "      <td>2.9</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>5.4</td>\n",
              "      <td>3.7</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>4.8</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>4.8</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>4.3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>5.8</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>5.7</td>\n",
              "      <td>4.4</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.4</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>5.4</td>\n",
              "      <td>3.9</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.4</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.3</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>5.7</td>\n",
              "      <td>3.8</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.3</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.8</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.3</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>5.4</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.7</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.4</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.3</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.5</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>4.8</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.4</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>5.2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>5.2</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>6.9</td>\n",
              "      <td>3.2</td>\n",
              "      <td>5.7</td>\n",
              "      <td>2.3</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>5.6</td>\n",
              "      <td>2.8</td>\n",
              "      <td>4.9</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>7.7</td>\n",
              "      <td>2.8</td>\n",
              "      <td>6.7</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>6.3</td>\n",
              "      <td>2.7</td>\n",
              "      <td>4.9</td>\n",
              "      <td>1.8</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>6.7</td>\n",
              "      <td>3.3</td>\n",
              "      <td>5.7</td>\n",
              "      <td>2.1</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>7.2</td>\n",
              "      <td>3.2</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1.8</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>6.2</td>\n",
              "      <td>2.8</td>\n",
              "      <td>4.8</td>\n",
              "      <td>1.8</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>6.1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.9</td>\n",
              "      <td>1.8</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <td>6.4</td>\n",
              "      <td>2.8</td>\n",
              "      <td>5.6</td>\n",
              "      <td>2.1</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>129</th>\n",
              "      <td>7.2</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.8</td>\n",
              "      <td>1.6</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130</th>\n",
              "      <td>7.4</td>\n",
              "      <td>2.8</td>\n",
              "      <td>6.1</td>\n",
              "      <td>1.9</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131</th>\n",
              "      <td>7.9</td>\n",
              "      <td>3.8</td>\n",
              "      <td>6.4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>6.4</td>\n",
              "      <td>2.8</td>\n",
              "      <td>5.6</td>\n",
              "      <td>2.2</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>6.3</td>\n",
              "      <td>2.8</td>\n",
              "      <td>5.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>6.1</td>\n",
              "      <td>2.6</td>\n",
              "      <td>5.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135</th>\n",
              "      <td>7.7</td>\n",
              "      <td>3.0</td>\n",
              "      <td>6.1</td>\n",
              "      <td>2.3</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>136</th>\n",
              "      <td>6.3</td>\n",
              "      <td>3.4</td>\n",
              "      <td>5.6</td>\n",
              "      <td>2.4</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137</th>\n",
              "      <td>6.4</td>\n",
              "      <td>3.1</td>\n",
              "      <td>5.5</td>\n",
              "      <td>1.8</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>138</th>\n",
              "      <td>6.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.8</td>\n",
              "      <td>1.8</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>139</th>\n",
              "      <td>6.9</td>\n",
              "      <td>3.1</td>\n",
              "      <td>5.4</td>\n",
              "      <td>2.1</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140</th>\n",
              "      <td>6.7</td>\n",
              "      <td>3.1</td>\n",
              "      <td>5.6</td>\n",
              "      <td>2.4</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141</th>\n",
              "      <td>6.9</td>\n",
              "      <td>3.1</td>\n",
              "      <td>5.1</td>\n",
              "      <td>2.3</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142</th>\n",
              "      <td>5.8</td>\n",
              "      <td>2.7</td>\n",
              "      <td>5.1</td>\n",
              "      <td>1.9</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>6.8</td>\n",
              "      <td>3.2</td>\n",
              "      <td>5.9</td>\n",
              "      <td>2.3</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>6.7</td>\n",
              "      <td>3.3</td>\n",
              "      <td>5.7</td>\n",
              "      <td>2.5</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>6.7</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.2</td>\n",
              "      <td>2.3</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>6.3</td>\n",
              "      <td>2.5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.9</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>6.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>6.2</td>\n",
              "      <td>3.4</td>\n",
              "      <td>5.4</td>\n",
              "      <td>2.3</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>5.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.1</td>\n",
              "      <td>1.8</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>150 rows  5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     SepalLength  SepalWidth  PetalLength  PetalWidth            Name\n",
              "0            5.1         3.5          1.4         0.2     Iris-setosa\n",
              "1            4.9         3.0          1.4         0.2     Iris-setosa\n",
              "2            4.7         3.2          1.3         0.2     Iris-setosa\n",
              "3            4.6         3.1          1.5         0.2     Iris-setosa\n",
              "4            5.0         3.6          1.4         0.2     Iris-setosa\n",
              "5            5.4         3.9          1.7         0.4     Iris-setosa\n",
              "6            4.6         3.4          1.4         0.3     Iris-setosa\n",
              "7            5.0         3.4          1.5         0.2     Iris-setosa\n",
              "8            4.4         2.9          1.4         0.2     Iris-setosa\n",
              "9            4.9         3.1          1.5         0.1     Iris-setosa\n",
              "10           5.4         3.7          1.5         0.2     Iris-setosa\n",
              "11           4.8         3.4          1.6         0.2     Iris-setosa\n",
              "12           4.8         3.0          1.4         0.1     Iris-setosa\n",
              "13           4.3         3.0          1.1         0.1     Iris-setosa\n",
              "14           5.8         4.0          1.2         0.2     Iris-setosa\n",
              "15           5.7         4.4          1.5         0.4     Iris-setosa\n",
              "16           5.4         3.9          1.3         0.4     Iris-setosa\n",
              "17           5.1         3.5          1.4         0.3     Iris-setosa\n",
              "18           5.7         3.8          1.7         0.3     Iris-setosa\n",
              "19           5.1         3.8          1.5         0.3     Iris-setosa\n",
              "20           5.4         3.4          1.7         0.2     Iris-setosa\n",
              "21           5.1         3.7          1.5         0.4     Iris-setosa\n",
              "22           4.6         3.6          1.0         0.2     Iris-setosa\n",
              "23           5.1         3.3          1.7         0.5     Iris-setosa\n",
              "24           4.8         3.4          1.9         0.2     Iris-setosa\n",
              "25           5.0         3.0          1.6         0.2     Iris-setosa\n",
              "26           5.0         3.4          1.6         0.4     Iris-setosa\n",
              "27           5.2         3.5          1.5         0.2     Iris-setosa\n",
              "28           5.2         3.4          1.4         0.2     Iris-setosa\n",
              "29           4.7         3.2          1.6         0.2     Iris-setosa\n",
              "..           ...         ...          ...         ...             ...\n",
              "120          6.9         3.2          5.7         2.3  Iris-virginica\n",
              "121          5.6         2.8          4.9         2.0  Iris-virginica\n",
              "122          7.7         2.8          6.7         2.0  Iris-virginica\n",
              "123          6.3         2.7          4.9         1.8  Iris-virginica\n",
              "124          6.7         3.3          5.7         2.1  Iris-virginica\n",
              "125          7.2         3.2          6.0         1.8  Iris-virginica\n",
              "126          6.2         2.8          4.8         1.8  Iris-virginica\n",
              "127          6.1         3.0          4.9         1.8  Iris-virginica\n",
              "128          6.4         2.8          5.6         2.1  Iris-virginica\n",
              "129          7.2         3.0          5.8         1.6  Iris-virginica\n",
              "130          7.4         2.8          6.1         1.9  Iris-virginica\n",
              "131          7.9         3.8          6.4         2.0  Iris-virginica\n",
              "132          6.4         2.8          5.6         2.2  Iris-virginica\n",
              "133          6.3         2.8          5.1         1.5  Iris-virginica\n",
              "134          6.1         2.6          5.6         1.4  Iris-virginica\n",
              "135          7.7         3.0          6.1         2.3  Iris-virginica\n",
              "136          6.3         3.4          5.6         2.4  Iris-virginica\n",
              "137          6.4         3.1          5.5         1.8  Iris-virginica\n",
              "138          6.0         3.0          4.8         1.8  Iris-virginica\n",
              "139          6.9         3.1          5.4         2.1  Iris-virginica\n",
              "140          6.7         3.1          5.6         2.4  Iris-virginica\n",
              "141          6.9         3.1          5.1         2.3  Iris-virginica\n",
              "142          5.8         2.7          5.1         1.9  Iris-virginica\n",
              "143          6.8         3.2          5.9         2.3  Iris-virginica\n",
              "144          6.7         3.3          5.7         2.5  Iris-virginica\n",
              "145          6.7         3.0          5.2         2.3  Iris-virginica\n",
              "146          6.3         2.5          5.0         1.9  Iris-virginica\n",
              "147          6.5         3.0          5.2         2.0  Iris-virginica\n",
              "148          6.2         3.4          5.4         2.3  Iris-virginica\n",
              "149          5.9         3.0          5.1         1.8  Iris-virginica\n",
              "\n",
              "[150 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "87vyf_Cu2DfH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1969
        },
        "outputId": "785fc9e5-33c6-48d5-b99c-d0e51636acf3"
      },
      "cell_type": "code",
      "source": [
        "iris_data"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SepalLength</th>\n",
              "      <th>SepalWidth</th>\n",
              "      <th>PetalLength</th>\n",
              "      <th>PetalWidth</th>\n",
              "      <th>Name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5.4</td>\n",
              "      <td>3.9</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.4</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.3</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>4.4</td>\n",
              "      <td>2.9</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>5.4</td>\n",
              "      <td>3.7</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>4.8</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>4.8</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>4.3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>5.8</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>5.7</td>\n",
              "      <td>4.4</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.4</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>5.4</td>\n",
              "      <td>3.9</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.4</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.3</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>5.7</td>\n",
              "      <td>3.8</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.3</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.8</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.3</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>5.4</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.7</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.4</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.3</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.5</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>4.8</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.4</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>5.2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>5.2</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>6.9</td>\n",
              "      <td>3.2</td>\n",
              "      <td>5.7</td>\n",
              "      <td>2.3</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>5.6</td>\n",
              "      <td>2.8</td>\n",
              "      <td>4.9</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>7.7</td>\n",
              "      <td>2.8</td>\n",
              "      <td>6.7</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>6.3</td>\n",
              "      <td>2.7</td>\n",
              "      <td>4.9</td>\n",
              "      <td>1.8</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>6.7</td>\n",
              "      <td>3.3</td>\n",
              "      <td>5.7</td>\n",
              "      <td>2.1</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>7.2</td>\n",
              "      <td>3.2</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1.8</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>6.2</td>\n",
              "      <td>2.8</td>\n",
              "      <td>4.8</td>\n",
              "      <td>1.8</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>6.1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.9</td>\n",
              "      <td>1.8</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <td>6.4</td>\n",
              "      <td>2.8</td>\n",
              "      <td>5.6</td>\n",
              "      <td>2.1</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>129</th>\n",
              "      <td>7.2</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.8</td>\n",
              "      <td>1.6</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130</th>\n",
              "      <td>7.4</td>\n",
              "      <td>2.8</td>\n",
              "      <td>6.1</td>\n",
              "      <td>1.9</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131</th>\n",
              "      <td>7.9</td>\n",
              "      <td>3.8</td>\n",
              "      <td>6.4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>6.4</td>\n",
              "      <td>2.8</td>\n",
              "      <td>5.6</td>\n",
              "      <td>2.2</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>6.3</td>\n",
              "      <td>2.8</td>\n",
              "      <td>5.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>6.1</td>\n",
              "      <td>2.6</td>\n",
              "      <td>5.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135</th>\n",
              "      <td>7.7</td>\n",
              "      <td>3.0</td>\n",
              "      <td>6.1</td>\n",
              "      <td>2.3</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>136</th>\n",
              "      <td>6.3</td>\n",
              "      <td>3.4</td>\n",
              "      <td>5.6</td>\n",
              "      <td>2.4</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137</th>\n",
              "      <td>6.4</td>\n",
              "      <td>3.1</td>\n",
              "      <td>5.5</td>\n",
              "      <td>1.8</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>138</th>\n",
              "      <td>6.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.8</td>\n",
              "      <td>1.8</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>139</th>\n",
              "      <td>6.9</td>\n",
              "      <td>3.1</td>\n",
              "      <td>5.4</td>\n",
              "      <td>2.1</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140</th>\n",
              "      <td>6.7</td>\n",
              "      <td>3.1</td>\n",
              "      <td>5.6</td>\n",
              "      <td>2.4</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141</th>\n",
              "      <td>6.9</td>\n",
              "      <td>3.1</td>\n",
              "      <td>5.1</td>\n",
              "      <td>2.3</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142</th>\n",
              "      <td>5.8</td>\n",
              "      <td>2.7</td>\n",
              "      <td>5.1</td>\n",
              "      <td>1.9</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>6.8</td>\n",
              "      <td>3.2</td>\n",
              "      <td>5.9</td>\n",
              "      <td>2.3</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>6.7</td>\n",
              "      <td>3.3</td>\n",
              "      <td>5.7</td>\n",
              "      <td>2.5</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>6.7</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.2</td>\n",
              "      <td>2.3</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>6.3</td>\n",
              "      <td>2.5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.9</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>6.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>6.2</td>\n",
              "      <td>3.4</td>\n",
              "      <td>5.4</td>\n",
              "      <td>2.3</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>5.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.1</td>\n",
              "      <td>1.8</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>150 rows  5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     SepalLength  SepalWidth  PetalLength  PetalWidth            Name\n",
              "0            5.1         3.5          1.4         0.2     Iris-setosa\n",
              "1            4.9         3.0          1.4         0.2     Iris-setosa\n",
              "2            4.7         3.2          1.3         0.2     Iris-setosa\n",
              "3            4.6         3.1          1.5         0.2     Iris-setosa\n",
              "4            5.0         3.6          1.4         0.2     Iris-setosa\n",
              "5            5.4         3.9          1.7         0.4     Iris-setosa\n",
              "6            4.6         3.4          1.4         0.3     Iris-setosa\n",
              "7            5.0         3.4          1.5         0.2     Iris-setosa\n",
              "8            4.4         2.9          1.4         0.2     Iris-setosa\n",
              "9            4.9         3.1          1.5         0.1     Iris-setosa\n",
              "10           5.4         3.7          1.5         0.2     Iris-setosa\n",
              "11           4.8         3.4          1.6         0.2     Iris-setosa\n",
              "12           4.8         3.0          1.4         0.1     Iris-setosa\n",
              "13           4.3         3.0          1.1         0.1     Iris-setosa\n",
              "14           5.8         4.0          1.2         0.2     Iris-setosa\n",
              "15           5.7         4.4          1.5         0.4     Iris-setosa\n",
              "16           5.4         3.9          1.3         0.4     Iris-setosa\n",
              "17           5.1         3.5          1.4         0.3     Iris-setosa\n",
              "18           5.7         3.8          1.7         0.3     Iris-setosa\n",
              "19           5.1         3.8          1.5         0.3     Iris-setosa\n",
              "20           5.4         3.4          1.7         0.2     Iris-setosa\n",
              "21           5.1         3.7          1.5         0.4     Iris-setosa\n",
              "22           4.6         3.6          1.0         0.2     Iris-setosa\n",
              "23           5.1         3.3          1.7         0.5     Iris-setosa\n",
              "24           4.8         3.4          1.9         0.2     Iris-setosa\n",
              "25           5.0         3.0          1.6         0.2     Iris-setosa\n",
              "26           5.0         3.4          1.6         0.4     Iris-setosa\n",
              "27           5.2         3.5          1.5         0.2     Iris-setosa\n",
              "28           5.2         3.4          1.4         0.2     Iris-setosa\n",
              "29           4.7         3.2          1.6         0.2     Iris-setosa\n",
              "..           ...         ...          ...         ...             ...\n",
              "120          6.9         3.2          5.7         2.3  Iris-virginica\n",
              "121          5.6         2.8          4.9         2.0  Iris-virginica\n",
              "122          7.7         2.8          6.7         2.0  Iris-virginica\n",
              "123          6.3         2.7          4.9         1.8  Iris-virginica\n",
              "124          6.7         3.3          5.7         2.1  Iris-virginica\n",
              "125          7.2         3.2          6.0         1.8  Iris-virginica\n",
              "126          6.2         2.8          4.8         1.8  Iris-virginica\n",
              "127          6.1         3.0          4.9         1.8  Iris-virginica\n",
              "128          6.4         2.8          5.6         2.1  Iris-virginica\n",
              "129          7.2         3.0          5.8         1.6  Iris-virginica\n",
              "130          7.4         2.8          6.1         1.9  Iris-virginica\n",
              "131          7.9         3.8          6.4         2.0  Iris-virginica\n",
              "132          6.4         2.8          5.6         2.2  Iris-virginica\n",
              "133          6.3         2.8          5.1         1.5  Iris-virginica\n",
              "134          6.1         2.6          5.6         1.4  Iris-virginica\n",
              "135          7.7         3.0          6.1         2.3  Iris-virginica\n",
              "136          6.3         3.4          5.6         2.4  Iris-virginica\n",
              "137          6.4         3.1          5.5         1.8  Iris-virginica\n",
              "138          6.0         3.0          4.8         1.8  Iris-virginica\n",
              "139          6.9         3.1          5.4         2.1  Iris-virginica\n",
              "140          6.7         3.1          5.6         2.4  Iris-virginica\n",
              "141          6.9         3.1          5.1         2.3  Iris-virginica\n",
              "142          5.8         2.7          5.1         1.9  Iris-virginica\n",
              "143          6.8         3.2          5.9         2.3  Iris-virginica\n",
              "144          6.7         3.3          5.7         2.5  Iris-virginica\n",
              "145          6.7         3.0          5.2         2.3  Iris-virginica\n",
              "146          6.3         2.5          5.0         1.9  Iris-virginica\n",
              "147          6.5         3.0          5.2         2.0  Iris-virginica\n",
              "148          6.2         3.4          5.4         2.3  Iris-virginica\n",
              "149          5.9         3.0          5.1         1.8  Iris-virginica\n",
              "\n",
              "[150 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "metadata": {
        "id": "HEHcHNZf2XRh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "55ffd0c8-f9b8-4e60-df43-48112eb9bd8d"
      },
      "cell_type": "code",
      "source": [
        "iris.csv"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-6db04c082d11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0miris\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'iris' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "lzpAkjAr2ZzC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "f8f53659-5dbb-4d8b-8917-e1d601b3e601"
      },
      "cell_type": "code",
      "source": [
        "conda info -e\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-696386a7d083>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    conda info -e\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "OYmhrUJQjo6S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "986b7463-6d7e-4127-c672-c768fe6a4560"
      },
      "cell_type": "code",
      "source": [
        "!conda info -e\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: conda: command not found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "54tamdCIjs6Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "52e9ad3f-2aef-41f5-d883-2c0ab403392c"
      },
      "cell_type": "code",
      "source": [
        "!ls -al"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 16\n",
            "drwxr-xr-x 4 root root 4096 Sep 28 23:32 .\n",
            "drwxr-xr-x 1 root root 4096 Oct  6 13:24 ..\n",
            "drwxr-xr-x 4 root root 4096 Sep 28 23:11 .config\n",
            "drwxr-xr-x 2 root root 4096 Sep 28 23:32 sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "N1ldHUTYkWqO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "29348ea5-e53c-4ca6-d4fc-9ac63e177b1d"
      },
      "cell_type": "code",
      "source": [
        "!cat .config\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cat: .config: Is a directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zRJHG3wLkadB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "1de1a17c-2317-4cec-c055-57d70529df11"
      },
      "cell_type": "code",
      "source": [
        "!ls -l .config\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 16\n",
            "-rw-r--r-- 1 root root    7 Sep 28 23:11 active_config\n",
            "-rw-r--r-- 1 root root    0 Sep 28 23:11 config_sentinel\n",
            "drwxr-xr-x 2 root root 4096 Sep 28 23:11 configurations\n",
            "-rw------- 1 root root    5 Sep 28 23:11 gce\n",
            "drwxr-xr-x 3 root root 4096 Sep 28 23:11 logs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1z1vcqqIkenl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "510a926a-dae7-4186-8d47-ee0234416384"
      },
      "cell_type": "code",
      "source": [
        "!cat /etc/cpuinfo\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cat: /etc/cpuinfo: No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "v7GM4_34ly9m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        },
        "outputId": "436a66eb-6787-4c85-ec87-7e3e861afaed"
      },
      "cell_type": "code",
      "source": [
        "!cat /proc/cpuinfo"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processor\t: 0\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 63\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2300.000\n",
            "cache size\t: 46080 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 0\n",
            "initial apicid\t: 0\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm pti fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms xsaveopt arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 l1tf\n",
            "bogomips\t: 4600.00\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 1\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 63\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2300.000\n",
            "cache size\t: 46080 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 1\n",
            "initial apicid\t: 1\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm pti fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms xsaveopt arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 l1tf\n",
            "bogomips\t: 4600.00\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "w00XUL2FmERe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "08349515-40f6-4af8-d041-37b22b615dfc"
      },
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: nvidia-smi: command not found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "t_nTaaeomjV2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a9c1cd68-9b95-4e8e-fc4e-6fd21f541ef8"
      },
      "cell_type": "code",
      "source": [
        "!lspci | grep VGA"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: lspci: command not found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "827vVJ4Bmof2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "67b969ae-a34b-4f32-a9df-b62e8e3a27d9"
      },
      "cell_type": "code",
      "source": [
        "!uname -a"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linux 2b53adddc941 4.14.33+ #1 SMP Sat Aug 11 08:05:16 PDT 2018 x86_64 x86_64 x86_64 GNU/Linux\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "H7Suqy-WmzFm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c4bbc191-e22a-489c-b7a7-543982ff20f6"
      },
      "cell_type": "code",
      "source": [
        "!cat /etc/issue"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ubuntu 17.10 \\n \\l\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "315ltefJm7f_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5083
        },
        "outputId": "580dff54-412d-413e-8c94-f2d689ce8b47"
      },
      "cell_type": "code",
      "source": [
        "!dpkg -l"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Desired=Unknown/Install/Remove/Purge/Hold\n",
            "| Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend\n",
            "|/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad)\n",
            "||/ Name           Version      Architecture Description\n",
            "+++-==============-============-============-=================================\n",
            "ii  adduser        3.113+nmu3ub all          add and remove users and groups\n",
            "ii  apt            1.5.2        amd64        commandline package manager\n",
            "ii  apt-utils      1.5.2        amd64        package management related utilit\n",
            "ii  base-files     9.6ubuntu102 amd64        Debian base system miscellaneous \n",
            "ii  base-passwd    3.5.43       amd64        Debian base system master passwor\n",
            "ii  bash           4.4-5ubuntu1 amd64        GNU Bourne Again SHell\n",
            "ii  binutils       2.29.1-4ubun amd64        GNU assembler, linker and binary \n",
            "ii  binutils-commo 2.29.1-4ubun amd64        Common files for the GNU assemble\n",
            "ii  binutils-x86-6 2.29.1-4ubun amd64        GNU binary utilities, for x86-64-\n",
            "ii  blt            2.5.3+dfsg-3 amd64        graphics extension library for Tc\n",
            "ii  bsdutils       1:2.30.1-0ub amd64        basic utilities from 4.4BSD-Lite\n",
            "ii  build-essentia 12.4ubuntu1  amd64        Informational list of build-essen\n",
            "ii  bzip2          1.0.6-8.1    amd64        high-quality block-sorting file c\n",
            "ii  ca-certificate 20170717     all          Common CA certificates\n",
            "ii  coreutils      8.26-3ubuntu amd64        GNU core utilities\n",
            "ii  cpp            4:7.2.0-1ubu amd64        GNU C preprocessor (cpp)\n",
            "ii  cpp-7          7.2.0-8ubunt amd64        GNU C preprocessor\n",
            "ii  curl           7.55.1-1ubun amd64        command line tool for transferrin\n",
            "ii  dash           0.5.8-2.3ubu amd64        POSIX-compliant shell\n",
            "ii  debconf        1.5.63       all          Debian configuration management s\n",
            "ii  debianutils    4.8.2        amd64        Miscellaneous utilities specific \n",
            "ii  dh-python      2.20170125   all          Debian helper tools for packaging\n",
            "ii  diffutils      1:3.6-1      amd64        File comparison utilities\n",
            "ii  dpkg           1.18.24ubunt amd64        Debian package management system\n",
            "ii  dpkg-dev       1.18.24ubunt all          Debian package development tools\n",
            "ii  e2fslibs:amd64 1.43.5-1     amd64        ext2/ext3/ext4 file system librar\n",
            "ii  e2fsprogs      1.43.5-1     amd64        ext2/ext3/ext4 file system utilit\n",
            "ii  fdisk          2.30.1-0ubun amd64        collection of partitioning utilit\n",
            "ii  findutils      4.6.0+git+20 amd64        utilities for finding files--find\n",
            "ii  fontconfig-con 2.11.94-0ubu all          generic font configuration librar\n",
            "ii  fonts-dejavu-c 2.37-1       all          Vera font family derivate with ad\n",
            "ii  fonts-liberati 1:1.07.4-2   all          Fonts with the same metrics as Ti\n",
            "ii  g++            4:7.2.0-1ubu amd64        GNU C++ compiler\n",
            "ii  g++-7          7.2.0-8ubunt amd64        GNU C++ compiler\n",
            "ii  gcc            4:7.2.0-1ubu amd64        GNU C compiler\n",
            "ii  gcc-7          7.2.0-8ubunt amd64        GNU C compiler\n",
            "ii  gcc-7-base:amd 7.2.0-8ubunt amd64        GCC, the GNU Compiler Collection \n",
            "ii  gfortran       4:7.2.0-1ubu amd64        GNU Fortran 95 compiler\n",
            "ii  gfortran-7     7.2.0-8ubunt amd64        GNU Fortran compiler\n",
            "ii  git            1:2.14.1-1ub amd64        fast, scalable, distributed revis\n",
            "ii  git-man        1:2.14.1-1ub all          fast, scalable, distributed revis\n",
            "ii  gnupg          2.1.15-1ubun amd64        GNU privacy guard - a free PGP re\n",
            "ii  gnupg-agent    2.1.15-1ubun amd64        GNU privacy guard - cryptographic\n",
            "ii  google-perftoo 2.5-2.2ubunt all          command line utilities to analyze\n",
            "ii  gpgv           2.1.15-1ubun amd64        GNU privacy guard - signature ver\n",
            "ii  grep           3.1-2        amd64        GNU grep, egrep and fgrep\n",
            "ii  gzip           1.6-5ubuntu1 amd64        GNU compression utilities\n",
            "ii  hdf5-helpers   1.10.0-patch amd64        Hierarchical Data Format 5 (HDF5)\n",
            "ii  hostname       3.18         amd64        utility to set/show the host name\n",
            "ii  icu-devtools   57.1-6ubuntu amd64        Development utilities for Interna\n",
            "ii  init-system-he 1.49ubuntu1  all          helper tools for all init systems\n",
            "ii  libacl1:amd64  2.2.52-3buil amd64        Access control list shared librar\n",
            "ii  libaec-dev:amd 0.3.2-2      amd64        Development files for the Adaptiv\n",
            "ii  libaec0:amd64  0.3.2-2      amd64        Adaptive Entropy Coding library\n",
            "ii  libapt-inst2.0 1.5.2        amd64        deb package format runtime librar\n",
            "ii  libapt-pkg5.0: 1.5.2        amd64        package management runtime librar\n",
            "ii  libasan4:amd64 7.2.0-8ubunt amd64        AddressSanitizer -- a fast memory\n",
            "ii  libasn1-8-heim 7.4.0.dfsg.1 amd64        Heimdal Kerberos - ASN.1 library\n",
            "ii  libassuan0:amd 2.4.3-3      amd64        IPC library for the GnuPG compone\n",
            "ii  libatlas-base- 3.10.3-5     amd64        Automatically Tuned Linear Algebr\n",
            "ii  libatlas3-base 3.10.3-5     amd64        Automatically Tuned Linear Algebr\n",
            "ii  libatomic1:amd 7.2.0-8ubunt amd64        support library providing __atomi\n",
            "ii  libattr1:amd64 1:2.4.47-2bu amd64        Extended attribute shared library\n",
            "ii  libaudit-commo 1:2.7.7-1ubu all          Dynamic library for security audi\n",
            "ii  libaudit1:amd6 1:2.7.7-1ubu amd64        Dynamic library for security audi\n",
            "ii  libbinutils:am 2.29.1-4ubun amd64        GNU binary utilities (private sha\n",
            "ii  libblkid1:amd6 2.30.1-0ubun amd64        block device ID library\n",
            "ii  libbsd0:amd64  0.8.6-1      amd64        utility functions from BSD system\n",
            "ii  libbz2-1.0:amd 1.0.6-8.1    amd64        high-quality block-sorting file c\n",
            "ii  libc-bin       2.26-0ubuntu amd64        GNU C Library: Binaries\n",
            "ii  libc-dev-bin   2.26-0ubuntu amd64        GNU C Library: Development binari\n",
            "ii  libc6:amd64    2.26-0ubuntu amd64        GNU C Library: Shared libraries\n",
            "ii  libc6-dev:amd6 2.26-0ubuntu amd64        GNU C Library: Development Librar\n",
            "ii  libcap-ng0:amd 0.7.7-3build amd64        An alternate POSIX capabilities l\n",
            "ii  libcc1-0:amd64 7.2.0-8ubunt amd64        GCC cc1 plugin for GDB\n",
            "ii  libcilkrts5:am 7.2.0-8ubunt amd64        Intel Cilk Plus language extensio\n",
            "ii  libcomerr2:amd 1.43.5-1     amd64        common error description library\n",
            "ii  libcublas8.0:a 8.0.61-1     amd64        NVIDIA cuBLAS Library\n",
            "ii  libcudart8.0:a 8.0.61-1     amd64        NVIDIA CUDA Runtime Library\n",
            "ii  libcufft8.0:am 8.0.61-1     amd64        NVIDIA cuFFT Library\n",
            "ii  libcufftw8.0:a 8.0.61-1     amd64        NVIDIA cuFFTW Library\n",
            "ii  libcurand8.0:a 8.0.61-1     amd64        NVIDIA cuRAND Library\n",
            "ii  libcurl3:amd64 7.55.1-1ubun amd64        easy-to-use client-side URL trans\n",
            "ii  libcurl3-gnutl 7.55.1-1ubun amd64        easy-to-use client-side URL trans\n",
            "ii  libcusolver8.0 8.0.61-1     amd64        NVIDIA cuSOLVER Library\n",
            "ii  libdb5.3:amd64 5.3.28-13.1  amd64        Berkeley v5.3 Database Libraries \n",
            "ii  libdebconfclie 0.213ubuntu1 amd64        Debian Configuration Management S\n",
            "ii  libdpkg-perl   1.18.24ubunt all          Dpkg perl modules\n",
            "ii  libedit2:amd64 3.1-20170329 amd64        BSD editline and history librarie\n",
            "ii  liberror-perl  0.17024-1    all          Perl module for error/exception h\n",
            "ii  libexpat1:amd6 2.2.3-1      amd64        XML parsing C library - runtime l\n",
            "ii  libexpat1-dev: 2.2.3-1      amd64        XML parsing C library - developme\n",
            "ii  libfdisk1:amd6 2.30.1-0ubun amd64        fdisk partitioning library\n",
            "ii  libffi6:amd64  3.2.1-6      amd64        Foreign Function Interface librar\n",
            "ii  libfontconfig1 2.11.94-0ubu amd64        generic font configuration librar\n",
            "ii  libfontconfig1 2.11.94-0ubu amd64        generic font configuration librar\n",
            "ii  libfreetype6:a 2.8-0.2ubunt amd64        FreeType 2 font engine, shared li\n",
            "ii  libfreetype6-d 2.8-0.2ubunt amd64        FreeType 2 font engine, developme\n",
            "ii  libgcc-7-dev:a 7.2.0-8ubunt amd64        GCC support library (development \n",
            "ii  libgcc1:amd64  1:7.2.0-8ubu amd64        GCC support library\n",
            "ii  libgcrypt20:am 1.7.8-2ubunt amd64        LGPL Crypto library - runtime lib\n",
            "ii  libgdbm3:amd64 1.8.3-14     amd64        GNU dbm database routines (runtim\n",
            "ii  libgfortran-7- 7.2.0-8ubunt amd64        Runtime library for GNU Fortran a\n",
            "ii  libgfortran4:a 7.2.0-8ubunt amd64        Runtime library for GNU Fortran a\n",
            "ii  libglib2.0-0:a 2.54.1-1ubun amd64        GLib library of C routines\n",
            "ii  libgmp10:amd64 2:6.1.2+dfsg amd64        Multiprecision arithmetic library\n",
            "ii  libgnutls30:am 3.5.8-6ubunt amd64        GNU TLS library - main runtime li\n",
            "ii  libgomp1:amd64 7.2.0-8ubunt amd64        GCC OpenMP (GOMP) support library\n",
            "ii  libgoogle-perf 2.5-2.2ubunt amd64        libraries for CPU and heap analys\n",
            "ii  libgpg-error0: 1.27-3       amd64        library for common error values a\n",
            "ii  libgssapi-krb5 1.15.1-2     amd64        MIT Kerberos runtime libraries - \n",
            "ii  libgssapi3-hei 7.4.0.dfsg.1 amd64        Heimdal Kerberos - GSSAPI support\n",
            "ii  libhcrypto4-he 7.4.0.dfsg.1 amd64        Heimdal Kerberos - crypto library\n",
            "ii  libhdf5-100:am 1.10.0-patch amd64        Hierarchical Data Format 5 (HDF5)\n",
            "ii  libhdf5-cpp-10 1.10.0-patch amd64        Hierarchical Data Format 5 (HDF5)\n",
            "ii  libhdf5-dev    1.10.0-patch amd64        Hierarchical Data Format 5 (HDF5)\n",
            "ii  libheimbase1-h 7.4.0.dfsg.1 amd64        Heimdal Kerberos - Base library\n",
            "ii  libheimntlm0-h 7.4.0.dfsg.1 amd64        Heimdal Kerberos - NTLM support l\n",
            "ii  libhogweed4:am 3.3-2        amd64        low level cryptographic library (\n",
            "ii  libhx509-5-hei 7.4.0.dfsg.1 amd64        Heimdal Kerberos - X509 support l\n",
            "ii  libice6:amd64  2:1.0.9-2    amd64        X11 Inter-Client Exchange library\n",
            "ii  libicu-dev     57.1-6ubuntu amd64        Development files for Internation\n",
            "ii  libicu57:amd64 57.1-6ubuntu amd64        International Components for Unic\n",
            "ii  libidn11:amd64 1.33-2       amd64        GNU Libidn library, implementatio\n",
            "ii  libidn2-0:amd6 2.0.2-5      amd64        Internationalized domain names (I\n",
            "ii  libisl15:amd64 0.18-1       amd64        manipulating sets and relations o\n",
            "ii  libitm1:amd64  7.2.0-8ubunt amd64        GNU Transactional Memory Library\n",
            "ii  libjpeg-dev:am 8c-2ubuntu8  amd64        Independent JPEG Group's JPEG run\n",
            "ii  libjpeg-turbo8 1.5.2-0ubunt amd64        IJG JPEG compliant runtime librar\n",
            "ii  libjpeg-turbo8 1.5.2-0ubunt amd64        Development files for the IJG JPE\n",
            "ii  libjpeg8:amd64 8c-2ubuntu8  amd64        Independent JPEG Group's JPEG run\n",
            "ii  libjpeg8-dev:a 8c-2ubuntu8  amd64        Independent JPEG Group's JPEG run\n",
            "ii  libk5crypto3:a 1.15.1-2     amd64        MIT Kerberos runtime libraries - \n",
            "ii  libkeyutils1:a 1.5.9-9ubunt amd64        Linux Key Management Utilities (l\n",
            "ii  libkrb5-26-hei 7.4.0.dfsg.1 amd64        Heimdal Kerberos - libraries\n",
            "ii  libkrb5-3:amd6 1.15.1-2     amd64        MIT Kerberos runtime libraries\n",
            "ii  libkrb5support 1.15.1-2     amd64        MIT Kerberos runtime libraries - \n",
            "ii  libksba8:amd64 1.3.5-2      amd64        X.509 and CMS support library\n",
            "ii  liblapack-dev: 3.7.1-3ubunt amd64        Library of linear algebra routine\n",
            "ii  liblapack3:amd 3.7.1-3ubunt amd64        Library of linear algebra routine\n",
            "ii  libldap-2.4-2: 2.4.45+dfsg- amd64        OpenLDAP libraries\n",
            "ii  libldap-common 2.4.45+dfsg- all          OpenLDAP common files for librari\n",
            "ii  liblsan0:amd64 7.2.0-8ubunt amd64        LeakSanitizer -- a memory leak de\n",
            "ii  liblz4-1:amd64 0.0~r131-2ub amd64        Fast LZ compression algorithm lib\n",
            "ii  liblzma5:amd64 5.2.2-1.3    amd64        XZ-format compression library\n",
            "ii  libmount1:amd6 2.30.1-0ubun amd64        device mounting library\n",
            "ii  libmpc3:amd64  1.0.3-2      amd64        multiple precision complex floati\n",
            "ii  libmpdec2:amd6 2.4.2-1      amd64        library for decimal floating poin\n",
            "ii  libmpfr4:amd64 3.1.6-1      amd64        multiple precision floating-point\n",
            "ii  libmpx2:amd64  7.2.0-8ubunt amd64        Intel memory protection extension\n",
            "ii  libncurses5:am 6.0+20160625 amd64        shared libraries for terminal han\n",
            "ii  libncursesw5:a 6.0+20160625 amd64        shared libraries for terminal han\n",
            "ii  libnettle6:amd 3.3-2        amd64        low level cryptographic library (\n",
            "ii  libnpth0:amd64 1.5-2        amd64        replacement for GNU Pth using sys\n",
            "ii  libp11-kit0:am 0.23.7-3     amd64        library for loading and coordinat\n",
            "ii  libpam-modules 1.1.8-3.2ubu amd64        Pluggable Authentication Modules \n",
            "ii  libpam-modules 1.1.8-3.2ubu amd64        Pluggable Authentication Modules \n",
            "ii  libpam-runtime 1.1.8-3.2ubu all          Runtime support for the PAM libra\n",
            "ii  libpam0g:amd64 1.1.8-3.2ubu amd64        Pluggable Authentication Modules \n",
            "ii  libpcre3:amd64 2:8.39-5ubun amd64        Old Perl 5 Compatible Regular Exp\n",
            "ii  libperl5.26:am 5.26.0-8ubun amd64        shared Perl library\n",
            "ii  libpgm-5.2-0:a 5.2.122~dfsg amd64        OpenPGM shared library\n",
            "ii  libpng-dev:amd 1.6.34-1ubun amd64        PNG library - development (versio\n",
            "ii  libpng16-16:am 1.6.34-1ubun amd64        PNG library - runtime (version 1.\n",
            "ii  libpopt0:amd64 1.16-10      amd64        lib for parsing cmdline parameter\n",
            "ii  libprocps6:amd 2:3.3.12-1ub amd64        library for accessing process inf\n",
            "ii  libpsl5:amd64  0.18.0-2     amd64        Library for Public Suffix List (s\n",
            "ii  libpthread-stu 0.3-4        amd64        pthread stubs not provided by nat\n",
            "ii  libpython-dev: 2.7.14-2ubun amd64        header files and a static library\n",
            "ii  libpython-stdl 2.7.14-2ubun amd64        interactive high-level object-ori\n",
            "ii  libpython2.7:a 2.7.14-2ubun amd64        Shared Python runtime library (ve\n",
            "ii  libpython2.7-d 2.7.14-2ubun amd64        Header files and a static library\n",
            "ii  libpython2.7-m 2.7.14-2ubun amd64        Minimal subset of the Python lang\n",
            "ii  libpython2.7-s 2.7.14-2ubun amd64        Interactive high-level object-ori\n",
            "ii  libpython3-dev 3.6.3-0ubunt amd64        header files and a static library\n",
            "ii  libpython3-std 3.6.3-0ubunt amd64        interactive high-level object-ori\n",
            "ii  libpython3.6:a 3.6.3-1ubunt amd64        Shared Python runtime library (ve\n",
            "ii  libpython3.6-d 3.6.3-1ubunt amd64        Header files and a static library\n",
            "ii  libpython3.6-m 3.6.3-1ubunt amd64        Minimal subset of the Python lang\n",
            "ii  libpython3.6-s 3.6.3-1ubunt amd64        Interactive high-level object-ori\n",
            "ii  libquadmath0:a 7.2.0-8ubunt amd64        GCC Quad-Precision Math Library\n",
            "ii  libreadline7:a 7.0-0ubuntu2 amd64        GNU readline and history librarie\n",
            "ii  libroken18-hei 7.4.0.dfsg.1 amd64        Heimdal Kerberos - roken support \n",
            "ii  librtmp1:amd64 2.4+20151223 amd64        toolkit for RTMP streams (shared \n",
            "ii  libsasl2-2:amd 2.1.27~101-g amd64        Cyrus SASL - authentication abstr\n",
            "ii  libsasl2-modul 2.1.27~101-g amd64        Cyrus SASL - pluggable authentica\n",
            "ii  libselinux1:am 2.7-1        amd64        SELinux runtime shared libraries\n",
            "ii  libsemanage-co 2.7-2        all          Common files for SELinux policy m\n",
            "ii  libsemanage1:a 2.7-2        amd64        SELinux policy management library\n",
            "ii  libsepol1:amd6 2.7-1        amd64        SELinux library for manipulating \n",
            "ii  libsm6:amd64   2:1.2.2-1    amd64        X11 Session Management library\n",
            "ii  libsmartcols1: 2.30.1-0ubun amd64        smart column output alignment lib\n",
            "ii  libsodium18:am 1.0.13-1     amd64        Network communication, cryptograp\n",
            "ii  libsqlite3-0:a 3.19.3-3     amd64        SQLite 3 shared library\n",
            "ii  libss2:amd64   1.43.5-1     amd64        command-line interface parsing li\n",
            "ii  libssl1.0.0:am 1.0.2g-1ubun amd64        Secure Sockets Layer toolkit - sh\n",
            "ii  libstdc++-7-de 7.2.0-8ubunt amd64        GNU Standard C++ Library v3 (deve\n",
            "ii  libstdc++6:amd 7.2.0-8ubunt amd64        GNU Standard C++ Library v3\n",
            "ii  libsystemd0:am 234-2ubuntu1 amd64        systemd utility library\n",
            "ii  libsz2:amd64   0.3.2-2      amd64        Adaptive Entropy Coding library -\n",
            "ii  libtasn1-6:amd 4.12-2.1ubun amd64        Manage ASN.1 structures (runtime)\n",
            "ii  libtcl8.6:amd6 8.6.7+dfsg-1 amd64        Tcl (the Tool Command Language) v\n",
            "ii  libtcmalloc-mi 2.5-2.2ubunt amd64        efficient thread-caching malloc\n",
            "ii  libtinfo5:amd6 6.0+20160625 amd64        shared low-level terminfo library\n",
            "ii  libtk8.6:amd64 8.6.7-1ubunt amd64        Tk toolkit for Tcl and X11 v8.6 -\n",
            "ii  libtsan0:amd64 7.2.0-8ubunt amd64        ThreadSanitizer -- a Valgrind-bas\n",
            "ii  libubsan0:amd6 7.2.0-8ubunt amd64        UBSan -- undefined behaviour sani\n",
            "ii  libudev1:amd64 234-2ubuntu1 amd64        libudev shared library\n",
            "ii  libunistring0: 0.9.3-5.2ubu amd64        Unicode string library for C\n",
            "ii  libunwind8     1.1-4.1ubunt amd64        library to determine the call-cha\n",
            "ii  libuuid1:amd64 2.30.1-0ubun amd64        Universally Unique ID library\n",
            "ii  libwind0-heimd 7.4.0.dfsg.1 amd64        Heimdal Kerberos - stringprep imp\n",
            "ii  libx11-6:amd64 2:1.6.4-3    amd64        X11 client-side library\n",
            "ii  libx11-data    2:1.6.4-3    all          X11 client-side library\n",
            "ii  libx11-dev:amd 2:1.6.4-3    amd64        X11 client-side library (developm\n",
            "ii  libxau-dev:amd 1:1.0.8-1    amd64        X11 authorisation library (develo\n",
            "ii  libxau6:amd64  1:1.0.8-1    amd64        X11 authorisation library\n",
            "ii  libxcb1:amd64  1.12-1ubuntu amd64        X C Binding\n",
            "ii  libxcb1-dev:am 1.12-1ubuntu amd64        X C Binding, development files\n",
            "ii  libxdmcp-dev:a 1:1.1.2-3    amd64        X11 authorisation library (develo\n",
            "ii  libxdmcp6:amd6 1:1.1.2-3    amd64        X11 Display Manager Control Proto\n",
            "ii  libxext6:amd64 2:1.3.3-1    amd64        X11 miscellaneous extension libra\n",
            "ii  libxft-dev     2.3.2-1      amd64        FreeType-based font drawing libra\n",
            "ii  libxft2:amd64  2.3.2-1      amd64        FreeType-based font drawing libra\n",
            "ii  libxml2:amd64  2.9.4+dfsg1- amd64        GNOME XML library\n",
            "ii  libxml2-dev:am 2.9.4+dfsg1- amd64        Development files for the GNOME X\n",
            "ii  libxrender-dev 1:0.9.10-1   amd64        X Rendering Extension client libr\n",
            "ii  libxrender1:am 1:0.9.10-1   amd64        X Rendering Extension client libr\n",
            "ii  libxss1:amd64  1:1.2.2-1    amd64        X11 Screen Saver extension librar\n",
            "ii  libzmq5:amd64  4.2.1-4ubunt amd64        lightweight messaging kernel (sha\n",
            "ii  linux-libc-dev 4.13.0-46.51 amd64        Linux Kernel Headers for developm\n",
            "ii  locales        2.26-0ubuntu all          GNU C Library: National Language \n",
            "ii  login          1:4.2-3.2ubu amd64        system login tools\n",
            "ii  lsb-base       9.20160110ub all          Linux Standard Base init script f\n",
            "ii  make           4.1-9.1      amd64        utility for directing compilation\n",
            "ii  mawk           1.3.3-17ubun amd64        a pattern scanning and text proce\n",
            "ii  mime-support   3.60ubuntu1  all          MIME files 'mime.types' & 'mailca\n",
            "ii  mount          2.30.1-0ubun amd64        tools for mounting and manipulati\n",
            "ii  multiarch-supp 2.26-0ubuntu amd64        Transitional package to ensure mu\n",
            "ii  ncurses-base   6.0+20160625 all          basic terminal type definitions\n",
            "ii  ncurses-bin    6.0+20160625 amd64        terminal-related programs and man\n",
            "ii  openssh-client 1:7.5p1-10ub amd64        secure shell (SSH) client, for se\n",
            "ii  openssl        1.0.2g-1ubun amd64        Secure Sockets Layer toolkit - cr\n",
            "ii  p7zip          16.02+dfsg-4 amd64        7zr file archiver with high compr\n",
            "ii  p7zip-full     16.02+dfsg-4 amd64        7z and 7za file archivers with hi\n",
            "ii  passwd         1:4.2-3.2ubu amd64        change and administer password an\n",
            "ii  patch          2.7.5-1ubunt amd64        Apply a diff file to an original\n",
            "ii  perl           5.26.0-8ubun amd64        Larry Wall's Practical Extraction\n",
            "ii  perl-base      5.26.0-8ubun amd64        minimal Perl system\n",
            "ii  perl-modules-5 5.26.0-8ubun all          Core Perl modules\n",
            "ii  pinentry-curse 1.0.0-2      amd64        curses-based PIN or pass-phrase e\n",
            "ii  pkg-config     0.29.1-0ubun amd64        manage compile and link flags for\n",
            "ii  procps         2:3.3.12-1ub amd64        /proc file system utilities\n",
            "ii  python         2.7.14-2ubun amd64        interactive high-level object-ori\n",
            "ii  python-dev     2.7.14-2ubun amd64        header files and a static library\n",
            "ii  python-minimal 2.7.14-2ubun amd64        minimal subset of the Python lang\n",
            "ii  python-tk      2.7.14-1     amd64        Tkinter - Writing Tk applications\n",
            "ii  python2.7      2.7.14-2ubun amd64        Interactive high-level object-ori\n",
            "ii  python2.7-dev  2.7.14-2ubun amd64        Header files and a static library\n",
            "ii  python2.7-mini 2.7.14-2ubun amd64        Minimal subset of the Python lang\n",
            "ii  python3        3.6.3-0ubunt amd64        interactive high-level object-ori\n",
            "ii  python3-dev    3.6.3-0ubunt amd64        header files and a static library\n",
            "ii  python3-minima 3.6.3-0ubunt amd64        minimal subset of the Python lang\n",
            "ii  python3-tk:amd 3.6.3-0ubunt amd64        Tkinter - Writing Tk applications\n",
            "ii  python3.6      3.6.3-1ubunt amd64        Interactive high-level object-ori\n",
            "ii  python3.6-dev  3.6.3-1ubunt amd64        Header files and a static library\n",
            "ii  python3.6-mini 3.6.3-1ubunt amd64        Minimal subset of the Python lang\n",
            "ii  readline-commo 7.0-0ubuntu2 all          GNU readline and history librarie\n",
            "ii  rename         0.20-6       all          Perl extension for renaming multi\n",
            "ii  rsync          3.1.2-2ubunt amd64        fast, versatile, remote (and loca\n",
            "ii  sed            4.4-1        amd64        GNU stream editor for filtering/t\n",
            "ii  sensible-utils 0.0.10ubuntu all          Utilities for sensible alternativ\n",
            "ii  sysvinit-utils 2.88dsf-59.8 amd64        System-V-like utilities\n",
            "ii  tar            1.29b-2      amd64        GNU version of the tar archiving \n",
            "ii  tk8.6-blt2.5   2.5.3+dfsg-3 amd64        graphics extension library for Tc\n",
            "ii  ttf-liberation 1:1.07.4-2   all          transitional dummy package\n",
            "ii  tzdata         2017c-0ubunt all          time zone and daylight-saving tim\n",
            "ii  ubuntu-keyring 2016.10.27   all          GnuPG keys of the Ubuntu archive\n",
            "ii  ucf            3.0036       all          Update Configuration File(s): pre\n",
            "ii  unzip          6.0-21ubuntu amd64        De-archiver for .zip files\n",
            "ii  util-linux     2.30.1-0ubun amd64        miscellaneous system utilities\n",
            "ii  wget           1.19.1-3ubun amd64        retrieves files from the web\n",
            "ii  x11-common     1:7.7+19ubun all          X Window System (X.Org) infrastru\n",
            "ii  x11proto-core- 7.0.31-1     all          X11 core wire protocol and auxili\n",
            "ii  x11proto-input 2.3.2-1      all          X11 Input extension wire protocol\n",
            "ii  x11proto-kb-de 1.0.7-1      all          X11 XKB extension wire protocol\n",
            "ii  x11proto-rende 2:0.11.1-2   all          X11 Render extension wire protoco\n",
            "ii  xorg-sgml-doct 1:1.11-1     all          Common tools for building X.Org S\n",
            "ii  xtrans-dev     1.3.5-1      all          X transport library (development \n",
            "ii  xz-utils       5.2.2-1.3    amd64        XZ-format compression utilities\n",
            "ii  zip            3.0-11build1 amd64        Archiver for .zip files\n",
            "ii  zlib1g:amd64   1:1.2.11.dfs amd64        compression library - runtime\n",
            "ii  zlib1g-dev:amd 1:1.2.11.dfs amd64        compression library - development\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7so3_pxjn6IR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "94eed861-64a8-4788-8634-01b668b80b17"
      },
      "cell_type": "code",
      "source": [
        "dpkg -l | grep nvidia"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-739ee7bc31f1>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    dpkg -l | grep nvidia\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "-4vTJgT3oSn2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3d416f4b-820e-43b7-f1b4-4b2a31121cd2"
      },
      "cell_type": "code",
      "source": [
        "!dpkg -l | grep cuda"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ii  libcudart8.0:amd64          8.0.61-1                          amd64        NVIDIA CUDA Runtime Library\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NjExFFUIoU1e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a6a4f434-d5c8-4270-fd60-ae76d05b2a91"
      },
      "cell_type": "code",
      "source": [
        "!pip install -q keras\n",
        "import keras"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "0H3idKCIoxC-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1751
        },
        "outputId": "05ae0932-930a-437c-9c77-1ff59ec70d70"
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "mnist.load_data()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((array([[[0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          ...,\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0]],\n",
              "  \n",
              "         [[0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          ...,\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0]],\n",
              "  \n",
              "         [[0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          ...,\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0]],\n",
              "  \n",
              "         ...,\n",
              "  \n",
              "         [[0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          ...,\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0]],\n",
              "  \n",
              "         [[0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          ...,\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0]],\n",
              "  \n",
              "         [[0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          ...,\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8),\n",
              "  array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)),\n",
              " (array([[[0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          ...,\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0]],\n",
              "  \n",
              "         [[0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          ...,\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0]],\n",
              "  \n",
              "         [[0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          ...,\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0]],\n",
              "  \n",
              "         ...,\n",
              "  \n",
              "         [[0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          ...,\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0]],\n",
              "  \n",
              "         [[0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          ...,\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0]],\n",
              "  \n",
              "         [[0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          ...,\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0],\n",
              "          [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8),\n",
              "  array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "kFaze2b8o6tW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e8648f25-7745-4f15-ba83-744a90e101bf"
      },
      "cell_type": "code",
      "source": [
        "!cat /proc/driver/nvidia/gpus/0000:00:04.0/information"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cat: '/proc/driver/nvidia/gpus/0000:00:04.0/information': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "waSbBLigpT_F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9a83c615-50c9-470b-e533-e3266c2be768"
      },
      "cell_type": "code",
      "source": [
        "!python -V\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 3.6.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "92vmdjD6pdJn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "51475644-ce4d-4a2f-aa49-b619ac2e7553"
      },
      "cell_type": "code",
      "source": [
        "!pip install -q http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl\n",
        "!pip install torchvision"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchvision\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K    100% || 61kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision) (0.3.0.post4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Collecting pillow>=4.1.1 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/94/5430ebaa83f91cc7a9f687ff5238e26164a779cca2ef9903232268b0a318/Pillow-5.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% || 2.0MB 9.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from torch->torchvision) (3.13)\n",
            "Installing collected packages: pillow, torchvision\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.3.0 torchvision-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OSRkgcitpi6w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8a4b7001-2d1a-411a-8af5-f71a56bd407a"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "8AGjt9KPp7gu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fL_WQ4zbp_oM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "9ad3f6d2-194d-47e3-9f2e-5eb9ffd7aec7"
      },
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2eaca526b1e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "TGng3xRwqDTH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0dd87d83-85ce-4021-ec0f-f4657a06632a"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "print(tensorflow.test.is_built_with_cuda())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "icps86AXqyuG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "da1a4dca-6c50-435f-d71c-38c83f6428ee"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4,shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "d_jk7ua7rt_1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N-5LC_jEsLZ8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "bb349391-1388-4715-93dd-9fec147461a5"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "# 1\n",
        "weights = np.zeros(10000) # 10000\n",
        "weights[300] = 1. # 300\n",
        "num_samples = 4 # \n",
        "\n",
        "# WeightedRandomSampler\n",
        "my_sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, num_samples, replacement=True)\n",
        "my_testloader = torch.utils.data.DataLoader(testset, batch_size=4,shuffle=False, num_workers=2, sampler=my_sampler)\n",
        "\n",
        "my_testiter = iter(my_testloader)\n",
        "images, labels = my_testiter.next()\n",
        "\n",
        "# imshow\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "imshow(torchvision.utils.make_grid(images))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd8AAACWCAYAAACfIIJIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQRAEwWFk8hUEQRAEh5HJVxAEQRAc5v8DrtzyEmzitCcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f2a05cf8d30>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "v4MywFpksWTH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        },
        "outputId": "765c0f3d-dae0-4673-a806-77cf0f6c5a97"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "\n",
        "# functions to show an image\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# print labels\n",
        "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "horse plane horse  bird\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAB5CAYAAAD2xKAmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nU40MAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f2a0587c630>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "waea4hONtXRz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}